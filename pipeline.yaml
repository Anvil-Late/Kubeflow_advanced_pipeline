apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: emission-prediction-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.10, pipelines.kubeflow.org/pipeline_compilation_time: '2022-01-28T10:26:04.914908',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example pipeline.",
      "inputs": [{"name": "bucket"}, {"name": "data_2015"}, {"name": "data_2016"},
      {"name": "hyperopt_iterations"}, {"name": "subfolder"}], "name": "Emission prediction
      pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.10}
spec:
  entrypoint: emission-prediction-pipeline
  templates:
  - name: emission-prediction-pipeline
    inputs:
      parameters:
      - {name: bucket}
      - {name: data_2015}
      - {name: data_2016}
      - {name: hyperopt_iterations}
      - {name: subfolder}
    dag:
      tasks:
      - name: evaluate-models
        template: evaluate-models
        dependencies: [train-randomforest, train-svm, train-xgb]
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: subfolder, value: '{{inputs.parameters.subfolder}}'}
          - {name: train-randomforest-MSE, value: '{{tasks.train-randomforest.outputs.parameters.train-randomforest-MSE}}'}
          - {name: train-randomforest-R2, value: '{{tasks.train-randomforest.outputs.parameters.train-randomforest-R2}}'}
          - {name: train-randomforest-hyperparams, value: '{{tasks.train-randomforest.outputs.parameters.train-randomforest-hyperparams}}'}
          - {name: train-svm-MSE, value: '{{tasks.train-svm.outputs.parameters.train-svm-MSE}}'}
          - {name: train-svm-R2, value: '{{tasks.train-svm.outputs.parameters.train-svm-R2}}'}
          - {name: train-svm-hyperparams, value: '{{tasks.train-svm.outputs.parameters.train-svm-hyperparams}}'}
          - {name: train-xgb-MSE, value: '{{tasks.train-xgb.outputs.parameters.train-xgb-MSE}}'}
          - {name: train-xgb-R2, value: '{{tasks.train-xgb.outputs.parameters.train-xgb-R2}}'}
          - {name: train-xgb-hyperparams, value: '{{tasks.train-xgb.outputs.parameters.train-xgb-hyperparams}}'}
      - name: merge-and-split
        template: merge-and-split
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: data_2015, value: '{{inputs.parameters.data_2015}}'}
          - {name: data_2016, value: '{{inputs.parameters.data_2016}}'}
      - name: model-predict
        template: model-predict
        dependencies: [prepare-data, train-best-model]
        arguments:
          artifacts:
          - {name: prepare-data-output_xtestcsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_xtestcsv}}'}
          - {name: train-best-model-output_pickle_model, from: '{{tasks.train-best-model.outputs.artifacts.train-best-model-output_pickle_model}}'}
      - name: prepare-data
        template: prepare-data
        dependencies: [preprocess-dataset]
        arguments:
          artifacts:
          - {name: preprocess-dataset-output_cleandatacsv, from: '{{tasks.preprocess-dataset.outputs.artifacts.preprocess-dataset-output_cleandatacsv}}'}
      - name: preprocess-dataset
        template: preprocess-dataset
        dependencies: [merge-and-split]
        arguments:
          artifacts:
          - {name: merge-and-split-output_edfcsv, from: '{{tasks.merge-and-split.outputs.artifacts.merge-and-split-output_edfcsv}}'}
      - name: train-best-model
        template: train-best-model
        dependencies: [evaluate-models, prepare-data]
        arguments:
          parameters:
          - {name: evaluate-models-best_model, value: '{{tasks.evaluate-models.outputs.parameters.evaluate-models-best_model}}'}
          - {name: evaluate-models-hyperparams, value: '{{tasks.evaluate-models.outputs.parameters.evaluate-models-hyperparams}}'}
          artifacts:
          - {name: prepare-data-output_xtraincsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_xtraincsv}}'}
          - {name: prepare-data-output_ytraincsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_ytraincsv}}'}
      - name: train-randomforest
        template: train-randomforest
        dependencies: [prepare-data]
        arguments:
          parameters:
          - {name: hyperopt_iterations, value: '{{inputs.parameters.hyperopt_iterations}}'}
          artifacts:
          - {name: prepare-data-output_xtestcsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_xtestcsv}}'}
          - {name: prepare-data-output_xtraincsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_xtraincsv}}'}
          - {name: prepare-data-output_ytestcsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_ytestcsv}}'}
          - {name: prepare-data-output_ytraincsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_ytraincsv}}'}
      - name: train-svm
        template: train-svm
        dependencies: [prepare-data]
        arguments:
          parameters:
          - {name: hyperopt_iterations, value: '{{inputs.parameters.hyperopt_iterations}}'}
          artifacts:
          - {name: prepare-data-output_xtestcsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_xtestcsv}}'}
          - {name: prepare-data-output_xtraincsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_xtraincsv}}'}
          - {name: prepare-data-output_ytestcsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_ytestcsv}}'}
          - {name: prepare-data-output_ytraincsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_ytraincsv}}'}
      - name: train-xgb
        template: train-xgb
        dependencies: [prepare-data]
        arguments:
          parameters:
          - {name: hyperopt_iterations, value: '{{inputs.parameters.hyperopt_iterations}}'}
          artifacts:
          - {name: prepare-data-output_xtestcsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_xtestcsv}}'}
          - {name: prepare-data-output_xtraincsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_xtraincsv}}'}
          - {name: prepare-data-output_ytestcsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_ytestcsv}}'}
          - {name: prepare-data-output_ytraincsv, from: '{{tasks.prepare-data.outputs.artifacts.prepare-data-output_ytraincsv}}'}
  - name: evaluate-models
    container:
      args: [--bucket, '{{inputs.parameters.bucket}}', --subfolder, '{{inputs.parameters.subfolder}}',
        --svm-mse, '{{inputs.parameters.train-svm-MSE}}', --svm-r2, '{{inputs.parameters.train-svm-R2}}',
        --svm-hyperparams, '{{inputs.parameters.train-svm-hyperparams}}', --xgb-mse,
        '{{inputs.parameters.train-xgb-MSE}}', --xgb-r2, '{{inputs.parameters.train-xgb-R2}}',
        --xgb-hyperparams, '{{inputs.parameters.train-xgb-hyperparams}}', --rf-mse,
        '{{inputs.parameters.train-randomforest-MSE}}', --rf-r2, '{{inputs.parameters.train-randomforest-R2}}',
        --rf-hyperparams, '{{inputs.parameters.train-randomforest-hyperparams}}',
        '----output-paths', /tmp/outputs/best_model/data, /tmp/outputs/hyperparams/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def evaluate_models(bucket, \n                     subfolder,\n         \
        \            svm_mse, \n                     svm_r2, \n                  \
        \   svm_hyperparams, \n                     xgb_mse, \n                  \
        \   xgb_r2, \n                     xgb_hyperparams, \n                   \
        \  rf_mse, \n                     rf_r2, \n                     rf_hyperparams\n\
        \                     ):\n\n    import pandas as pd\n    import matplotlib.pyplot\
        \ as plt\n    import seaborn as sns\n    import numpy as np\n    import boto3\n\
        \n    def easy_bar_plot(x, y, data, figname, order=None, xlab=None, ylab=None,\
        \ title=None, grid=True,\n                  values_over_bars=True, vob_round=0,\
        \ vob_offset=None, vob_rot=None, x_tick_rot=None):\n\n        fig, ax = plt.subplots(figsize\
        \ = (18, 8))\n        if order is None:\n            order = np.sort(data[x].unique())\
        \ \n        sns.barplot(x=x, y=y, data=data, ax=ax, order=order)\n       \
        \ if xlab is not None:\n            ax.set_xlabel(xlab, fontsize = 16, fontweight\
        \ = \"bold\")\n        if ylab is not None:\n            ax.set_ylabel(ylab,\
        \ fontsize = 16, fontweight = \"bold\")\n        if title is not None:\n \
        \           plt.suptitle(title, fontsize = 18, fontweight = \"bold\")\n\n\
        \        if grid :\n            plt.grid(b=True, which='major', axis='both',\
        \ alpha = 0.3)\n\n        if values_over_bars:\n            if vob_offset\
        \ is None:\n                vob_offset = 0.015\n            if vob_rot is\
        \ None:\n                vob_rot = 0\n            if vob_rot > 0:\n      \
        \          ha=\"left\"\n            else:\n                ha=\"center\"\n\
        \            pos=0\n            for i, (q, val) in data.iterrows():\n    \
        \            if val != 0:\n                    ax.text(pos, val + vob_offset*data[y].max(),\
        \ \"{}\".format(round(val,vob_round)), \n                            ha=ha,\
        \ fontsize = 12, fontweight = \"bold\", rotation=vob_rot, \n             \
        \              rotation_mode=\"anchor\")\n                pos += 1\n     \
        \   if x_tick_rot is not None:\n            plt.xticks(rotation = x_tick_rot,\
        \ ha=\"right\")\n        plt.savefig(figname)\n        plt.show()\n\n    performance_report\
        \ = {}\n    performance_report[\"SVM\"] = {\"MSE\" : svm_mse, \"R2\" : svm_r2}\n\
        \    performance_report[\"XGB\"] = {\"MSE\" : xgb_mse, \"R2\" : xgb_r2}\n\
        \    performance_report[\"RandomForest\"] = {\"MSE\" : rf_mse, \"R2\" : rf_r2}\n\
        \n    performance_df = pd.DataFrame.from_dict(performance_report, orient=\"\
        index\")\n    performance_df = performance_df.reset_index().rename(columns={\"\
        index\" : \"Model\"})\n\n    hyperparams_dict = {\"SVM\" : svm_hyperparams,\
        \ \"XGB\" : xgb_hyperparams, \"RandomForest\" : rf_hyperparams}\n\n    easy_bar_plot(x=\"\
        Model\", y=\"R2\", data=performance_df[[\"Model\", \"R2\"]],\n           \
        \       figname = \"./model_performance_R2.png\",\n                  order=performance_df[\"\
        Model\"], \n                  xlab=\"Model\", \n                  ylab=\"\
        R2 score\", \n                  title=\"R2 score by Model\", \n          \
        \        grid=True, \n                  values_over_bars=True, \n        \
        \          vob_round=3, \n                  vob_offset=None, \n          \
        \        vob_rot=None, \n                  x_tick_rot=None)\n\n    easy_bar_plot(x=\"\
        Model\", y=\"MSE\", data=performance_df[[\"Model\", \"MSE\"]], \n        \
        \          figname = \"./model_performance_MSE.png\",\n                  order=performance_df[\"\
        Model\"], \n                  xlab=\"Model\", \n                  ylab=\"\
        MSE\", \n                  title=\"MSE by Model\", \n                  grid=True,\
        \ \n                  values_over_bars=True, \n                  vob_round=3,\
        \ \n                  vob_offset=None, \n                  vob_rot=None, \n\
        \                  x_tick_rot=None)\n\n    best_model = performance_df.loc[performance_df[\"\
        R2\"]==performance_df[\"R2\"].max(), \"Model\"].values[0]\n    best_models_hyperparams\
        \ = hyperparams_dict[best_model]\n\n    s3_resource = boto3.client('s3')\n\
        \    s3_resource.upload_file(\"./model_performance_R2.png\", \n          \
        \                  bucket, \n                            subfolder + \"/model_performance_R2.png\"\
        )\n    s3_resource.upload_file(\"./model_performance_MSE.png\", \n       \
        \                     bucket, \n                            subfolder + \"\
        /model_performance_MSE.png\")\n\n    return (best_model, best_models_hyperparams)\n\
        \ndef _serialize_json(obj) -> str:\n    if isinstance(obj, str):\n       \
        \ return obj\n    import json\n\n    def default_serializer(obj):\n      \
        \  if hasattr(obj, 'to_struct'):\n            return obj.to_struct()\n   \
        \     else:\n            raise TypeError(\n                \"Object of type\
        \ '%s' is not JSON serializable and does not have .to_struct() method.\"\n\
        \                % obj.__class__.__name__)\n\n    return json.dumps(obj, default=default_serializer,\
        \ sort_keys=True)\n\ndef _serialize_str(str_value: str) -> str:\n    if not\
        \ isinstance(str_value, str):\n        raise TypeError('Value \"{}\" has type\
        \ \"{}\" instead of str.'.format(\n            str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Evaluate\
        \ models', description='')\n_parser.add_argument(\"--bucket\", dest=\"bucket\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --subfolder\", dest=\"subfolder\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--svm-mse\", dest=\"svm_mse\", type=float, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--svm-r2\", dest=\"svm_r2\"\
        , type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --svm-hyperparams\", dest=\"svm_hyperparams\", type=json.loads, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--xgb-mse\", dest=\"\
        xgb_mse\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --xgb-r2\", dest=\"xgb_r2\", type=float, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--xgb-hyperparams\", dest=\"xgb_hyperparams\", type=json.loads,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--rf-mse\"\
        , dest=\"rf_mse\", type=float, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--rf-r2\", dest=\"rf_r2\", type=float, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--rf-hyperparams\", dest=\"\
        rf_hyperparams\", type=json.loads, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = evaluate_models(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_str,\n    _serialize_json,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest
    inputs:
      parameters:
      - {name: bucket}
      - {name: subfolder}
      - {name: train-randomforest-MSE}
      - {name: train-randomforest-R2}
      - {name: train-randomforest-hyperparams}
      - {name: train-svm-MSE}
      - {name: train-svm-R2}
      - {name: train-svm-hyperparams}
      - {name: train-xgb-MSE}
      - {name: train-xgb-R2}
      - {name: train-xgb-hyperparams}
    outputs:
      parameters:
      - name: evaluate-models-best_model
        valueFrom: {path: /tmp/outputs/best_model/data}
      - name: evaluate-models-hyperparams
        valueFrom: {path: /tmp/outputs/hyperparams/data}
      artifacts:
      - {name: evaluate-models-best_model, path: /tmp/outputs/best_model/data}
      - {name: evaluate-models-hyperparams, path: /tmp/outputs/hyperparams/data}
    metadata:
      annotations: {author: Antoine Villatte, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--bucket", {"inputValue": "bucket"}, "--subfolder",
          {"inputValue": "subfolder"}, "--svm-mse", {"inputValue": "svm_mse"}, "--svm-r2",
          {"inputValue": "svm_r2"}, "--svm-hyperparams", {"inputValue": "svm_hyperparams"},
          "--xgb-mse", {"inputValue": "xgb_mse"}, "--xgb-r2", {"inputValue": "xgb_r2"},
          "--xgb-hyperparams", {"inputValue": "xgb_hyperparams"}, "--rf-mse", {"inputValue":
          "rf_mse"}, "--rf-r2", {"inputValue": "rf_r2"}, "--rf-hyperparams", {"inputValue":
          "rf_hyperparams"}, "----output-paths", {"outputPath": "best_model"}, {"outputPath":
          "hyperparams"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def evaluate_models(bucket, \n                     subfolder,\n                     svm_mse,
          \n                     svm_r2, \n                     svm_hyperparams, \n                     xgb_mse,
          \n                     xgb_r2, \n                     xgb_hyperparams, \n                     rf_mse,
          \n                     rf_r2, \n                     rf_hyperparams\n                     ):\n\n    import
          pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as
          sns\n    import numpy as np\n    import boto3\n\n    def easy_bar_plot(x,
          y, data, figname, order=None, xlab=None, ylab=None, title=None, grid=True,\n                  values_over_bars=True,
          vob_round=0, vob_offset=None, vob_rot=None, x_tick_rot=None):\n\n        fig,
          ax = plt.subplots(figsize = (18, 8))\n        if order is None:\n            order
          = np.sort(data[x].unique()) \n        sns.barplot(x=x, y=y, data=data, ax=ax,
          order=order)\n        if xlab is not None:\n            ax.set_xlabel(xlab,
          fontsize = 16, fontweight = \"bold\")\n        if ylab is not None:\n            ax.set_ylabel(ylab,
          fontsize = 16, fontweight = \"bold\")\n        if title is not None:\n            plt.suptitle(title,
          fontsize = 18, fontweight = \"bold\")\n\n        if grid :\n            plt.grid(b=True,
          which=''major'', axis=''both'', alpha = 0.3)\n\n        if values_over_bars:\n            if
          vob_offset is None:\n                vob_offset = 0.015\n            if
          vob_rot is None:\n                vob_rot = 0\n            if vob_rot >
          0:\n                ha=\"left\"\n            else:\n                ha=\"center\"\n            pos=0\n            for
          i, (q, val) in data.iterrows():\n                if val != 0:\n                    ax.text(pos,
          val + vob_offset*data[y].max(), \"{}\".format(round(val,vob_round)), \n                            ha=ha,
          fontsize = 12, fontweight = \"bold\", rotation=vob_rot, \n                           rotation_mode=\"anchor\")\n                pos
          += 1\n        if x_tick_rot is not None:\n            plt.xticks(rotation
          = x_tick_rot, ha=\"right\")\n        plt.savefig(figname)\n        plt.show()\n\n    performance_report
          = {}\n    performance_report[\"SVM\"] = {\"MSE\" : svm_mse, \"R2\" : svm_r2}\n    performance_report[\"XGB\"]
          = {\"MSE\" : xgb_mse, \"R2\" : xgb_r2}\n    performance_report[\"RandomForest\"]
          = {\"MSE\" : rf_mse, \"R2\" : rf_r2}\n\n    performance_df = pd.DataFrame.from_dict(performance_report,
          orient=\"index\")\n    performance_df = performance_df.reset_index().rename(columns={\"index\"
          : \"Model\"})\n\n    hyperparams_dict = {\"SVM\" : svm_hyperparams, \"XGB\"
          : xgb_hyperparams, \"RandomForest\" : rf_hyperparams}\n\n    easy_bar_plot(x=\"Model\",
          y=\"R2\", data=performance_df[[\"Model\", \"R2\"]],\n                  figname
          = \"./model_performance_R2.png\",\n                  order=performance_df[\"Model\"],
          \n                  xlab=\"Model\", \n                  ylab=\"R2 score\",
          \n                  title=\"R2 score by Model\", \n                  grid=True,
          \n                  values_over_bars=True, \n                  vob_round=3,
          \n                  vob_offset=None, \n                  vob_rot=None, \n                  x_tick_rot=None)\n\n    easy_bar_plot(x=\"Model\",
          y=\"MSE\", data=performance_df[[\"Model\", \"MSE\"]], \n                  figname
          = \"./model_performance_MSE.png\",\n                  order=performance_df[\"Model\"],
          \n                  xlab=\"Model\", \n                  ylab=\"MSE\", \n                  title=\"MSE
          by Model\", \n                  grid=True, \n                  values_over_bars=True,
          \n                  vob_round=3, \n                  vob_offset=None, \n                  vob_rot=None,
          \n                  x_tick_rot=None)\n\n    best_model = performance_df.loc[performance_df[\"R2\"]==performance_df[\"R2\"].max(),
          \"Model\"].values[0]\n    best_models_hyperparams = hyperparams_dict[best_model]\n\n    s3_resource
          = boto3.client(''s3'')\n    s3_resource.upload_file(\"./model_performance_R2.png\",
          \n                            bucket, \n                            subfolder
          + \"/model_performance_R2.png\")\n    s3_resource.upload_file(\"./model_performance_MSE.png\",
          \n                            bucket, \n                            subfolder
          + \"/model_performance_MSE.png\")\n\n    return (best_model, best_models_hyperparams)\n\ndef
          _serialize_json(obj) -> str:\n    if isinstance(obj, str):\n        return
          obj\n    import json\n\n    def default_serializer(obj):\n        if hasattr(obj,
          ''to_struct''):\n            return obj.to_struct()\n        else:\n            raise
          TypeError(\n                \"Object of type ''%s'' is not JSON serializable
          and does not have .to_struct() method.\"\n                % obj.__class__.__name__)\n\n    return
          json.dumps(obj, default=default_serializer, sort_keys=True)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(\n            str(str_value),
          str(type(str_value))))\n    return str_value\n\nimport json\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Evaluate models'', description='''')\n_parser.add_argument(\"--bucket\",
          dest=\"bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--subfolder\",
          dest=\"subfolder\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--svm-mse\",
          dest=\"svm_mse\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--svm-r2\",
          dest=\"svm_r2\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--svm-hyperparams\",
          dest=\"svm_hyperparams\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--xgb-mse\",
          dest=\"xgb_mse\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--xgb-r2\",
          dest=\"xgb_r2\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--xgb-hyperparams\",
          dest=\"xgb_hyperparams\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--rf-mse\",
          dest=\"rf_mse\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--rf-r2\",
          dest=\"rf_r2\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--rf-hyperparams\",
          dest=\"rf_hyperparams\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = evaluate_models(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n    _serialize_json,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest"}}, "inputs": [{"name":
          "bucket", "type": "String"}, {"name": "subfolder", "type": "String"}, {"name":
          "svm_mse", "type": "Float"}, {"name": "svm_r2", "type": "Float"}, {"name":
          "svm_hyperparams", "type": "JsonObject"}, {"name": "xgb_mse", "type": "Float"},
          {"name": "xgb_r2", "type": "Float"}, {"name": "xgb_hyperparams", "type":
          "JsonObject"}, {"name": "rf_mse", "type": "Float"}, {"name": "rf_r2", "type":
          "Float"}, {"name": "rf_hyperparams", "type": "JsonObject"}], "metadata":
          {"annotations": {"author": "Antoine Villatte"}}, "name": "Evaluate models",
          "outputs": [{"name": "best_model", "type": "String"}, {"name": "hyperparams",
          "type": "JsonObject"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "71480ea4e663a7a4488dd1308150bd5a38c8e976615901e4bab658e8670afc1a", "url":
          "./kf_utils/evaluate_models_op.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"bucket":
          "{{inputs.parameters.bucket}}", "rf_hyperparams": "{{inputs.parameters.train-randomforest-hyperparams}}",
          "rf_mse": "{{inputs.parameters.train-randomforest-MSE}}", "rf_r2": "{{inputs.parameters.train-randomforest-R2}}",
          "subfolder": "{{inputs.parameters.subfolder}}", "svm_hyperparams": "{{inputs.parameters.train-svm-hyperparams}}",
          "svm_mse": "{{inputs.parameters.train-svm-MSE}}", "svm_r2": "{{inputs.parameters.train-svm-R2}}",
          "xgb_hyperparams": "{{inputs.parameters.train-xgb-hyperparams}}", "xgb_mse":
          "{{inputs.parameters.train-xgb-MSE}}", "xgb_r2": "{{inputs.parameters.train-xgb-R2}}"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: merge-and-split
    container:
      args: [--bucket, '{{inputs.parameters.bucket}}', --data-2015, '{{inputs.parameters.data_2015}}',
        --data-2016, '{{inputs.parameters.data_2016}}', --output-edfcsv, /tmp/outputs/output_edfcsv/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef merge_and_split(bucket, \n                    data_2015, \n        \
        \            data_2016, \n                    output_edfcsv):\n\n    import\
        \ pandas as pd\n    import re\n    import boto3\n    from io import StringIO\n\
        \    from sklearn.model_selection import GroupShuffleSplit\n\n    def get_location(text,\
        \ method):\n        \"\"\"Retrieves data from the 2015 dataset to harmonize\
        \ it with the 2016 dataset\"\"\"\n        if method == \"latitude\":\n   \
        \         match = re.search('{\\'latitude\\': \\'(.+?)\\', \\'longitude',\
        \ text)\n        elif method == \"longitude\":\n            match = re.search('[0-9]\\\
        ', \\'longitude\\': \\'(.+?)\\', \\'human_address\\'', text)\n        elif\
        \ method == \"address\":\n            match = re.search('\\'{\"address\":\
        \ \"(.+?)\", \"city\":', text)      \n        elif method == \"city\":\n \
        \           match = re.search('\", \"city\": \"(.+?)\", \"state\":', text)\n\
        \        elif method == \"state\":\n            match = re.search('\"state\"\
        : \"(.+?)\", \"zip\":', text)\n        elif method == \"zip\":\n         \
        \   match = re.search('\"zip\": \"(.+?)\"}\\'}', text)\n        else:\n  \
        \          raise ValueError(\"Veuillez choisir une m\xE9thode (latitude, longitude,\
        \ adress, city, state, zip)\")\n\n        if match:\n            found = match.group(1)\n\
        \            return(found)\n        return(\"N/A\")\n\n    # Load datasets\n\
        \    csv_strings = {}\n    encoding = 'utf-8'\n    for source_data, year in\
        \ zip([data_2015, data_2016], [\"2015\", \"2016\"]):\n        csv_obj = boto3.client('s3').get_object(Bucket=bucket,\
        \ Key=source_data)\n        body = csv_obj['Body']\n        csv_string = body.read().decode(encoding)\n\
        \        csv_strings[year] = csv_string\n\n    data15 = pd.read_csv(StringIO(csv_strings[\"\
        2015\"]))\n    data16 = pd.read_csv(StringIO(csv_strings[\"2016\"]))\n\n \
        \   # Rename mismatched columns\n    rename_cols = {\n        \"GHGEmissions(MetricTonsCO2e)\"\
        \ : \"TotalGHGEmissions\",\n        \"GHGEmissionsIntensity(kgCO2e/ft2)\"\
        \ : \"GHGEmissionsIntensity\",\n        \"Comment\" : \"Comments\"\n    }\n\
        \    data15.rename(columns = rename_cols, inplace = True)\n\n    # Extract\
        \ location info from 2015 dataset to harmonize it\n    data15[\"Latitude\"\
        ] = data15[\"Location\"].apply(get_location, method = \"latitude\")\n    data15[\"\
        Longitude\"] = data15[\"Location\"].apply(get_location, method = \"longitude\"\
        )\n    data15[\"Address\"] = data15[\"Location\"].apply(get_location, method\
        \ = \"address\")\n    data15[\"City\"] = data15[\"Location\"].apply(get_location,\
        \ method = \"city\")\n    data15[\"State\"] = data15[\"Location\"].apply(get_location,\
        \ method = \"state\")\n    data15[\"ZipCode\"] = data15[\"Location\"].apply(get_location,\
        \ method = \"zip\")\n    data15[\"ZipCode\"] = data15[\"ZipCode\"].astype(int)\
        \ # convert to numeric\n    data15.drop(columns = \"Location\", inplace =\
        \ True)\n\n    # Delete columns from data15 that arent in data16\n    data15.drop(columns\
        \ = set(data15.columns.tolist()).difference(data16.columns.tolist()), inplace\
        \ = True)\n\n    # Harmonize column order\n    cols_order = data16.columns.tolist()\n\
        \    data15 = data15.loc[:, cols_order]\n\n    # Concatenate\n    emission_df\
        \ = pd.concat([data15, data16], axis = 0, ignore_index = True)\n\n    # Train/test\
        \ split\n    inTrain , inTest = next(GroupShuffleSplit(train_size = 0.7, random_state\
        \ = 42).\\\n                        split(emission_df, groups = emission_df[\"\
        OSEBuildingID\"]))\n\n    emission_df[\"in_train\"] = 0\n    emission_df.iloc[inTrain,\
        \ 46] = 1\n\n    emission_df.to_csv(output_edfcsv, index=False)\n\nimport\
        \ argparse\n_parser = argparse.ArgumentParser(prog='Merge and split', description='')\n\
        _parser.add_argument(\"--bucket\", dest=\"bucket\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-2015\", dest=\"\
        data_2015\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --data-2016\", dest=\"data_2016\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-edfcsv\", dest=\"output_edfcsv\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = merge_and_split(**_parsed_args)\n"
      image: public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest
    inputs:
      parameters:
      - {name: bucket}
      - {name: data_2015}
      - {name: data_2016}
    outputs:
      artifacts:
      - {name: merge-and-split-output_edfcsv, path: /tmp/outputs/output_edfcsv/data}
    metadata:
      annotations: {author: Antoine Villatte, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--bucket", {"inputValue": "bucket"}, "--data-2015",
          {"inputValue": "data_2015"}, "--data-2016", {"inputValue": "data_2016"},
          "--output-edfcsv", {"outputPath": "output_edfcsv"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef merge_and_split(bucket, \n                    data_2015,
          \n                    data_2016, \n                    output_edfcsv):\n\n    import
          pandas as pd\n    import re\n    import boto3\n    from io import StringIO\n    from
          sklearn.model_selection import GroupShuffleSplit\n\n    def get_location(text,
          method):\n        \"\"\"Retrieves data from the 2015 dataset to harmonize
          it with the 2016 dataset\"\"\"\n        if method == \"latitude\":\n            match
          = re.search(''{\\''latitude\\'': \\''(.+?)\\'', \\''longitude'', text)\n        elif
          method == \"longitude\":\n            match = re.search(''[0-9]\\'', \\''longitude\\'':
          \\''(.+?)\\'', \\''human_address\\'''', text)\n        elif method == \"address\":\n            match
          = re.search(''\\''{\"address\": \"(.+?)\", \"city\":'', text)      \n        elif
          method == \"city\":\n            match = re.search(''\", \"city\": \"(.+?)\",
          \"state\":'', text)\n        elif method == \"state\":\n            match
          = re.search(''\"state\": \"(.+?)\", \"zip\":'', text)\n        elif method
          == \"zip\":\n            match = re.search(''\"zip\": \"(.+?)\"}\\''}'',
          text)\n        else:\n            raise ValueError(\"Veuillez choisir une
          m\u00e9thode (latitude, longitude, adress, city, state, zip)\")\n\n        if
          match:\n            found = match.group(1)\n            return(found)\n        return(\"N/A\")\n\n    #
          Load datasets\n    csv_strings = {}\n    encoding = ''utf-8''\n    for source_data,
          year in zip([data_2015, data_2016], [\"2015\", \"2016\"]):\n        csv_obj
          = boto3.client(''s3'').get_object(Bucket=bucket, Key=source_data)\n        body
          = csv_obj[''Body'']\n        csv_string = body.read().decode(encoding)\n        csv_strings[year]
          = csv_string\n\n    data15 = pd.read_csv(StringIO(csv_strings[\"2015\"]))\n    data16
          = pd.read_csv(StringIO(csv_strings[\"2016\"]))\n\n    # Rename mismatched
          columns\n    rename_cols = {\n        \"GHGEmissions(MetricTonsCO2e)\" :
          \"TotalGHGEmissions\",\n        \"GHGEmissionsIntensity(kgCO2e/ft2)\" :
          \"GHGEmissionsIntensity\",\n        \"Comment\" : \"Comments\"\n    }\n    data15.rename(columns
          = rename_cols, inplace = True)\n\n    # Extract location info from 2015
          dataset to harmonize it\n    data15[\"Latitude\"] = data15[\"Location\"].apply(get_location,
          method = \"latitude\")\n    data15[\"Longitude\"] = data15[\"Location\"].apply(get_location,
          method = \"longitude\")\n    data15[\"Address\"] = data15[\"Location\"].apply(get_location,
          method = \"address\")\n    data15[\"City\"] = data15[\"Location\"].apply(get_location,
          method = \"city\")\n    data15[\"State\"] = data15[\"Location\"].apply(get_location,
          method = \"state\")\n    data15[\"ZipCode\"] = data15[\"Location\"].apply(get_location,
          method = \"zip\")\n    data15[\"ZipCode\"] = data15[\"ZipCode\"].astype(int)
          # convert to numeric\n    data15.drop(columns = \"Location\", inplace =
          True)\n\n    # Delete columns from data15 that arent in data16\n    data15.drop(columns
          = set(data15.columns.tolist()).difference(data16.columns.tolist()), inplace
          = True)\n\n    # Harmonize column order\n    cols_order = data16.columns.tolist()\n    data15
          = data15.loc[:, cols_order]\n\n    # Concatenate\n    emission_df = pd.concat([data15,
          data16], axis = 0, ignore_index = True)\n\n    # Train/test split\n    inTrain
          , inTest = next(GroupShuffleSplit(train_size = 0.7, random_state = 42).\\\n                        split(emission_df,
          groups = emission_df[\"OSEBuildingID\"]))\n\n    emission_df[\"in_train\"]
          = 0\n    emission_df.iloc[inTrain, 46] = 1\n\n    emission_df.to_csv(output_edfcsv,
          index=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Merge
          and split'', description='''')\n_parser.add_argument(\"--bucket\", dest=\"bucket\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-2015\",
          dest=\"data_2015\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-2016\",
          dest=\"data_2016\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-edfcsv\",
          dest=\"output_edfcsv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = merge_and_split(**_parsed_args)\n"], "image": "public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest"}},
          "inputs": [{"name": "bucket", "type": "String"}, {"name": "data_2015", "type":
          "String"}, {"name": "data_2016", "type": "String"}], "metadata": {"annotations":
          {"author": "Antoine Villatte"}}, "name": "Merge and split", "outputs": [{"name":
          "output_edfcsv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "f3a4193e175a44e4d99167846361ada3bc260fdaf0563200bbbd47d5ba43e260", "url":
          "./kf_utils/merge_and_split_op.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"bucket":
          "{{inputs.parameters.bucket}}", "data_2015": "{{inputs.parameters.data_2015}}",
          "data_2016": "{{inputs.parameters.data_2016}}"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: model-predict
    container:
      args: [--input-model-pickle, /tmp/inputs/input_model_pickle/data, --input-X-predict-csv,
        /tmp/inputs/input_X_predict_csv/data, --output-prediction-csv, /tmp/outputs/output_prediction_csv/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef model_predict(input_model_pickle, \n                  input_X_predict_csv,\n\
        \                  output_prediction_csv):\n\n    import pandas as pd\n  \
        \  import pickle\n    import numpy as np\n\n    with open(input_model_pickle,\
        \ 'rb') as file:\n        modfit = pickle.load(file)\n\n    X_predict = pd.read_csv(input_X_predict_csv)\n\
        \    predictions = modfit.predict(X_predict)\n    predictions = np.exp(predictions)\
        \ - 1\n\n    predictions_df = pd.DataFrame()\n    predictions_df[\"y_pred\"\
        ] = predictions\n\n    predictions_df.to_csv(output_prediction_csv)\n\nimport\
        \ argparse\n_parser = argparse.ArgumentParser(prog='Model predict', description='')\n\
        _parser.add_argument(\"--input-model-pickle\", dest=\"input_model_pickle\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --input-X-predict-csv\", dest=\"input_X_predict_csv\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-prediction-csv\"\
        , dest=\"output_prediction_csv\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = model_predict(**_parsed_args)\n"
      image: public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest
    inputs:
      artifacts:
      - {name: prepare-data-output_xtestcsv, path: /tmp/inputs/input_X_predict_csv/data}
      - {name: train-best-model-output_pickle_model, path: /tmp/inputs/input_model_pickle/data}
    outputs:
      artifacts:
      - {name: model-predict-output_prediction_csv, path: /tmp/outputs/output_prediction_csv/data}
    metadata:
      annotations: {author: Antoine Villatte, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--input-model-pickle", {"inputPath": "input_model_pickle"},
          "--input-X-predict-csv", {"inputPath": "input_X_predict_csv"}, "--output-prediction-csv",
          {"outputPath": "output_prediction_csv"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef model_predict(input_model_pickle,
          \n                  input_X_predict_csv,\n                  output_prediction_csv):\n\n    import
          pandas as pd\n    import pickle\n    import numpy as np\n\n    with open(input_model_pickle,
          ''rb'') as file:\n        modfit = pickle.load(file)\n\n    X_predict =
          pd.read_csv(input_X_predict_csv)\n    predictions = modfit.predict(X_predict)\n    predictions
          = np.exp(predictions) - 1\n\n    predictions_df = pd.DataFrame()\n    predictions_df[\"y_pred\"]
          = predictions\n\n    predictions_df.to_csv(output_prediction_csv)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Model predict'', description='''')\n_parser.add_argument(\"--input-model-pickle\",
          dest=\"input_model_pickle\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-X-predict-csv\",
          dest=\"input_X_predict_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-prediction-csv\",
          dest=\"output_prediction_csv\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = model_predict(**_parsed_args)\n"], "image": "public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest"}},
          "inputs": [{"name": "input_model_pickle", "type": "Pickle"}, {"name": "input_X_predict_csv",
          "type": "CSV"}], "metadata": {"annotations": {"author": "Antoine Villatte"}},
          "name": "Model predict", "outputs": [{"name": "output_prediction_csv", "type":
          "CSV"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "f4721521d1bbab4a9793e687253d927d5360eb52b40b3cc66b4940bd464dffbe",
          "url": "./kf_utils/model_predict_op.yaml"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: prepare-data
    container:
      args: [--input-cleandatacsv, /tmp/inputs/input_cleandatacsv/data, --output-xtraincsv,
        /tmp/outputs/output_xtraincsv/data, --output-ytraincsv, /tmp/outputs/output_ytraincsv/data,
        --output-xtestcsv, /tmp/outputs/output_xtestcsv/data, --output-ytestcsv, /tmp/outputs/output_ytestcsv/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef prepare_data(input_cleandatacsv,\n                output_xtraincsv,\n\
        \                output_ytraincsv,\n                output_xtestcsv,\n   \
        \             output_ytestcsv):\n\n    import numpy as np\n    import pandas\
        \ as pd\n    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n\
        \n    clean_data = pd.read_csv(input_cleandatacsv)\n\n    # Numeric / categorical\
        \ separation\n    numcols = clean_data.select_dtypes(include = np.number).columns\n\
        \    catcols = list(set(clean_data.columns) - set(numcols))\n    othercols\
        \ = [\"in_train\", \"SiteEnergyUse(kBtu)\", \"TotalGHGEmissions\", \"BCResponse\"\
        ]\n    numcols = [feature for feature in numcols.tolist() if feature not in\
        \ othercols]\n\n    num_data = clean_data.loc[:, numcols]\n    cat_data =\
        \ clean_data.loc[:, catcols]\n    rest_data = clean_data.loc[:, othercols]\n\
        \n    # Normalize\n    standardscaler = StandardScaler()\n    num_data = standardscaler.fit_transform(num_data)\n\
        \    num_data = pd.DataFrame(num_data, columns = numcols)\n\n    # Delete\
        \ outliers\n    not_extreme = (num_data.lt(3).all(axis = 1)) & (num_data.gt(-3).all(axis\
        \ = 1))\n    for index in range(len(not_extreme)):\n        if rest_data.loc[:,\
        \ \"in_train\"].iloc[index] == 0:\n            not_extreme[index] = True\n\
        \n    # Encode categorical variables\n    ohencoder = OneHotEncoder()\n  \
        \  ohecat_data = ohencoder.fit_transform(cat_data).toarray()\n    ohecat_colnames\
        \ = []\n    for feature in catcols:\n        for value in clean_data[feature].unique().tolist():\n\
        \            ohecat_colnames.append(feature + \"_\" + value)\n    ohecat_data\
        \ = pd.DataFrame(ohecat_data, columns = ohecat_colnames)\n\n    # Remove sparse\
        \ dummies \n    ohecat_data = ohecat_data.loc[:, ohecat_data.sum().ge(30)]\n\
        \n    # Concatenate\n    preproc_data = pd.concat([rest_data, num_data, ohecat_data],\
        \ axis = 1)\n    preproc_data = preproc_data.loc[not_extreme, :]\n    rest_data\
        \ = rest_data.loc[not_extreme, :]\n\n    # train-test split\n    train = preproc_data.loc[preproc_data[\"\
        in_train\"] == 1, :].drop(columns = \"in_train\")\n    test = preproc_data.loc[preproc_data[\"\
        in_train\"] == 0, :].drop(columns = \"in_train\")\n\n    train = train.drop(columns\
        \ = [\"TotalGHGEmissions\", \"SiteEnergyUse(kBtu)\"])\n    test = test.drop(columns\
        \ = [\"TotalGHGEmissions\", \"SiteEnergyUse(kBtu)\"])\n\n    # Separate features\
        \ from preds\n    X_train = train.drop(columns = \"BCResponse\")\n    Y_train\
        \ = train[\"BCResponse\"]\n\n    X_test = test.drop(columns = \"BCResponse\"\
        )\n    Y_test = test[\"BCResponse\"]\n\n    # Save files \n    output_artifact_list\
        \ = [output_xtraincsv, output_ytraincsv, output_xtestcsv, output_ytestcsv]\n\
        \n    for index, dataset in enumerate([X_train, Y_train, X_test, Y_test]):\n\
        \        artif = output_artifact_list[index]\n        dataset.to_csv(artif,\
        \ index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Prepare\
        \ data', description='')\n_parser.add_argument(\"--input-cleandatacsv\", dest=\"\
        input_cleandatacsv\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-xtraincsv\", dest=\"output_xtraincsv\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-ytraincsv\"\
        , dest=\"output_ytraincsv\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-xtestcsv\",\
        \ dest=\"output_xtestcsv\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-ytestcsv\",\
        \ dest=\"output_ytestcsv\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = prepare_data(**_parsed_args)\n"
      image: public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest
    inputs:
      artifacts:
      - {name: preprocess-dataset-output_cleandatacsv, path: /tmp/inputs/input_cleandatacsv/data}
    outputs:
      artifacts:
      - {name: prepare-data-output_xtestcsv, path: /tmp/outputs/output_xtestcsv/data}
      - {name: prepare-data-output_xtraincsv, path: /tmp/outputs/output_xtraincsv/data}
      - {name: prepare-data-output_ytestcsv, path: /tmp/outputs/output_ytestcsv/data}
      - {name: prepare-data-output_ytraincsv, path: /tmp/outputs/output_ytraincsv/data}
    metadata:
      annotations: {author: Antoine Villatte, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--input-cleandatacsv", {"inputPath": "input_cleandatacsv"},
          "--output-xtraincsv", {"outputPath": "output_xtraincsv"}, "--output-ytraincsv",
          {"outputPath": "output_ytraincsv"}, "--output-xtestcsv", {"outputPath":
          "output_xtestcsv"}, "--output-ytestcsv", {"outputPath": "output_ytestcsv"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef prepare_data(input_cleandatacsv,\n                output_xtraincsv,\n                output_ytraincsv,\n                output_xtestcsv,\n                output_ytestcsv):\n\n    import
          numpy as np\n    import pandas as pd\n    from sklearn.preprocessing import
          StandardScaler, OneHotEncoder\n\n    clean_data = pd.read_csv(input_cleandatacsv)\n\n    #
          Numeric / categorical separation\n    numcols = clean_data.select_dtypes(include
          = np.number).columns\n    catcols = list(set(clean_data.columns) - set(numcols))\n    othercols
          = [\"in_train\", \"SiteEnergyUse(kBtu)\", \"TotalGHGEmissions\", \"BCResponse\"]\n    numcols
          = [feature for feature in numcols.tolist() if feature not in othercols]\n\n    num_data
          = clean_data.loc[:, numcols]\n    cat_data = clean_data.loc[:, catcols]\n    rest_data
          = clean_data.loc[:, othercols]\n\n    # Normalize\n    standardscaler =
          StandardScaler()\n    num_data = standardscaler.fit_transform(num_data)\n    num_data
          = pd.DataFrame(num_data, columns = numcols)\n\n    # Delete outliers\n    not_extreme
          = (num_data.lt(3).all(axis = 1)) & (num_data.gt(-3).all(axis = 1))\n    for
          index in range(len(not_extreme)):\n        if rest_data.loc[:, \"in_train\"].iloc[index]
          == 0:\n            not_extreme[index] = True\n\n    # Encode categorical
          variables\n    ohencoder = OneHotEncoder()\n    ohecat_data = ohencoder.fit_transform(cat_data).toarray()\n    ohecat_colnames
          = []\n    for feature in catcols:\n        for value in clean_data[feature].unique().tolist():\n            ohecat_colnames.append(feature
          + \"_\" + value)\n    ohecat_data = pd.DataFrame(ohecat_data, columns =
          ohecat_colnames)\n\n    # Remove sparse dummies \n    ohecat_data = ohecat_data.loc[:,
          ohecat_data.sum().ge(30)]\n\n    # Concatenate\n    preproc_data = pd.concat([rest_data,
          num_data, ohecat_data], axis = 1)\n    preproc_data = preproc_data.loc[not_extreme,
          :]\n    rest_data = rest_data.loc[not_extreme, :]\n\n    # train-test split\n    train
          = preproc_data.loc[preproc_data[\"in_train\"] == 1, :].drop(columns = \"in_train\")\n    test
          = preproc_data.loc[preproc_data[\"in_train\"] == 0, :].drop(columns = \"in_train\")\n\n    train
          = train.drop(columns = [\"TotalGHGEmissions\", \"SiteEnergyUse(kBtu)\"])\n    test
          = test.drop(columns = [\"TotalGHGEmissions\", \"SiteEnergyUse(kBtu)\"])\n\n    #
          Separate features from preds\n    X_train = train.drop(columns = \"BCResponse\")\n    Y_train
          = train[\"BCResponse\"]\n\n    X_test = test.drop(columns = \"BCResponse\")\n    Y_test
          = test[\"BCResponse\"]\n\n    # Save files \n    output_artifact_list =
          [output_xtraincsv, output_ytraincsv, output_xtestcsv, output_ytestcsv]\n\n    for
          index, dataset in enumerate([X_train, Y_train, X_test, Y_test]):\n        artif
          = output_artifact_list[index]\n        dataset.to_csv(artif, index = False,
          header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Prepare
          data'', description='''')\n_parser.add_argument(\"--input-cleandatacsv\",
          dest=\"input_cleandatacsv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-xtraincsv\",
          dest=\"output_xtraincsv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-ytraincsv\",
          dest=\"output_ytraincsv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-xtestcsv\",
          dest=\"output_xtestcsv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-ytestcsv\",
          dest=\"output_ytestcsv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = prepare_data(**_parsed_args)\n"], "image": "public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest"}},
          "inputs": [{"name": "input_cleandatacsv", "type": "CSV"}], "metadata": {"annotations":
          {"author": "Antoine Villatte"}}, "name": "Prepare data", "outputs": [{"name":
          "output_xtraincsv", "type": "CSV"}, {"name": "output_ytraincsv", "type":
          "CSV"}, {"name": "output_xtestcsv", "type": "CSV"}, {"name": "output_ytestcsv",
          "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "9e63df7ee17b6e48d63b5ff6f40c06f9af81e0c2f4add240f355524c53c1219a",
          "url": "./kf_utils/prepare_data_op.yaml"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: preprocess-dataset
    container:
      args: [--input-edfcsv, /tmp/inputs/input_edfcsv/data, --output-cleandatacsv,
        /tmp/outputs/output_cleandatacsv/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef preprocess_dataset(input_edfcsv,\n                        output_cleandatacsv):\n\
        \n    import pandas as pd\n    import numpy as np\n    from scipy.stats import\
        \ boxcox\n    import re\n    import math\n\n    def count_commas(text):\n\
        \        \"\"\"Fonction text.count(\",\") sachant g\xE9rer les nan\"\"\"\n\
        \        if type(text) is str :\n            return(text.count(\",\") + 1)\n\
        \        else :\n            if np.isnan(text):\n                return(np.nan)\n\
        \            else :\n                raise(TypeError(\"Valeur non textuelle\
        \ et non NA observ\xE9e\"))\n\n    def fetch_noFloors(emission_df, neighborhood,\
        \ property_type):\n        neighb_prop_pivot_table = emission_df.dropna().loc[emission_df[\"\
        in_train\"] == 1, :].pivot_table(\"NumberofFloors\", \n                  \
        \                                                 index = \"Neighborhood\"\
        , \n                                                                   columns\
        \ = \"PrimaryPropertyType\", \n                                          \
        \                         aggfunc = lambda X: X.quantile(0.5))\n        output\
        \ = neighb_prop_pivot_table.loc[neighborhood, property_type]\n        if math.isnan(output):\n\
        \            print(\"Pas de donn\xE9es pour ce type de b\xE2timent dans ce\
        \ voisinage, extraction de la m\xE9diane globale\")\n            output =\
        \ round(neighb_prop_pivot_table.loc[:, property_type].quantile(0.5),0)\n\n\
        \        return(output)\n\n    emission_df = pd.read_csv(input_edfcsv)\n\n\
        \    # Log of response variable\n    emission_df[\"BCResponse\"] = boxcox(emission_df[\"\
        TotalGHGEmissions\"] + 1, -0.1)\n\n    # Fill NAs\n        ## Impute SecondLargestPropertyUseType\n\
        \    emission_df.loc[(emission_df[\"SecondLargestPropertyUseType\"].isna())\
        \ & (~emission_df[\"ListOfAllPropertyUseTypes\"].isna()), \n             \
        \       \"SecondLargestPropertyUseType\"] = \"None\"\n\n        ## Impute\
        \ SecondLargestPropertyUseTypeGFA\n    emission_df.loc[(emission_df[\"SecondLargestPropertyUseTypeGFA\"\
        ].isna()) & (~emission_df[\"ListOfAllPropertyUseTypes\"].isna()), \n     \
        \               \"SecondLargestPropertyUseTypeGFA\"] = 0\n\n        ## Impute\
        \ ThirdLargestPropertyUseType\n    emission_df.loc[(emission_df[\"ThirdLargestPropertyUseType\"\
        ].isna()) & (~emission_df[\"ListOfAllPropertyUseTypes\"].isna()), \n     \
        \               \"ThirdLargestPropertyUseType\"] = \"None\"\n\n        ##\
        \ Impute ThirdLargestPropertyUseTypeGFA\n    emission_df.loc[(emission_df[\"\
        ThirdLargestPropertyUseTypeGFA\"].isna()) & (~emission_df[\"ListOfAllPropertyUseTypes\"\
        ].isna()), \n                    \"ThirdLargestPropertyUseTypeGFA\"] = 0\n\
        \n    # Drop columns with too many NaNs\n    emission_df.drop(columns = [\"\
        YearsENERGYSTARCertified\", \"Comments\", \"Outlier\"], inplace = True)\n\n\
        \    # Only keep non residential buildings\n    emission_df = emission_df.loc[emission_df[\"\
        BuildingType\"].apply(lambda X: bool(re.search(\"^[Nn]on[Rr]esidential\",\
        \ X))), :]\n    # Drop Building Type\n    emission_df.drop(columns = \"BuildingType\"\
        , inplace = True)\n\n    # Drop low-impact location variables\n    emission_df.drop(columns\
        \ = [\"Address\", \"City\", \"State\", \"ZipCode\", \"TaxParcelIdentificationNumber\"\
        , \"CouncilDistrictCode\"], \n                    inplace = True)\n\n    #\
        \ Drop ID variables\n    emission_df.drop(columns = [\"OSEBuildingID\", \"\
        PropertyName\", \"DefaultData\"], inplace = True)\n\n    # Reset index\n \
        \   emission_df.reset_index(inplace = True, drop = True)\n\n    # Drop variables\
        \ too highly correlated to the reponse variable\n    emission_df.drop(columns\
        \ = ['SiteEUIWN(kBtu/sf)', 'SourceEUI(kBtu/sf)', 'SourceEUIWN(kBtu/sf)',\n\
        \                                'SiteEnergyUseWN(kBtu)', 'Electricity(kWh)',\
        \ 'NaturalGas(therms)', \n                                \"SiteEUI(kBtu/sf)\"\
        , \"GHGEmissionsIntensity\"], inplace = True)\n\n    # Feature engineering\
        \ : neighborhood type\n    emission_df[\"Neighborhood_type_GHGE\"] = \"med-low\"\
        \n    emission_df.loc[emission_df[\"Neighborhood\"].isin([\"GREATER DUWAMISH\"\
        , \"SOUTHEAST\"]), \"Neighborhood_type_GHGE\"] = \"low\"\n    emission_df.loc[emission_df[\"\
        Neighborhood\"].isin([\"NORTHEAST\", \"MAGNOLIA / QUEEN ANNE\", \"SOUTHWEST\"\
        , \"LAKE UNION\"]), \n                    \"Neighborhood_type_GHGE\"] = \"\
        med-high\"\n    emission_df.loc[emission_df[\"Neighborhood\"].isin([\"DOWNTOWN\"\
        , \"EAST\"]), \"Neighborhood_type_GHGE\"] = \"high\"\n\n    # Feature engineering\
        \ : age\n    emission_df[\"Age\"] = emission_df[\"DataYear\"] - emission_df[\"\
        YearBuilt\"]\n\n    # FE : Number of use types\n    emission_df[\"NumberOfUseTypes\"\
        ] = emission_df[\"ListOfAllPropertyUseTypes\"].apply(count_commas)\n\n   \
        \ # FE : Proportion occupied by each use\n    three_uses_sum = emission_df[\"\
        LargestPropertyUseTypeGFA\"] + emission_df[\"SecondLargestPropertyUseTypeGFA\"\
        ] + \\\n                    emission_df[\"ThirdLargestPropertyUseTypeGFA\"\
        ]\n\n    emission_df[\"PrimaryUseGFARatio\"] = round(emission_df[\"LargestPropertyUseTypeGFA\"\
        ] / three_uses_sum, 3)\n    emission_df[\"SecondaryUseGFARatio\"] = round(emission_df[\"\
        SecondLargestPropertyUseTypeGFA\"] / three_uses_sum, 3)\n    emission_df[\"\
        TerciaryUseGFARatio\"] = round(emission_df[\"ThirdLargestPropertyUseTypeGFA\"\
        ] / three_uses_sum, 3)\n\n    # FE : proportion of each energy use\n    emission_df[\"\
        NG_ratio\"] = round(emission_df[\"NaturalGas(kBtu)\"] / emission_df[\"SiteEnergyUse(kBtu)\"\
        ], 3)\n    emission_df[\"Elec_ratio\"] = round(emission_df[\"Electricity(kBtu)\"\
        ] / emission_df[\"SiteEnergyUse(kBtu)\"], 3)\n    emission_df[\"Steam_ratio\"\
        ] = round(emission_df[\"SteamUse(kBtu)\"] / emission_df[\"SiteEnergyUse(kBtu)\"\
        ], 3)\n    emission_df.drop(index = [780, 2781, 2027, 498, 1677], inplace\
        \ = True) # Impossible values\n    emission_df.reset_index(drop = True, inplace\
        \ = True)\n\n    # Drop useless columns\n    clean_data = emission_df.drop(columns\
        \ = [\"DataYear\", \"Neighborhood\", \"Longitude\", \"Latitude\", \"YearBuilt\"\
        , \"PropertyGFATotal\", \n                                            \"SteamUse(kBtu)\"\
        , \"Electricity(kBtu)\", \"NaturalGas(kBtu)\", \"PrimaryPropertyType\"])\n\
        \n    # Drop NAs\n    clean_data.dropna(subset = [\"LargestPropertyUseType\"\
        , \"TotalGHGEmissions\", \"NG_ratio\"], inplace = True)\n    emission_df.dropna().loc[emission_df[\"\
        in_train\"] == 1, :].pivot_table(\"NumberofFloors\", \n                  \
        \                                                      index = \"Neighborhood\"\
        , \n                                                                     \
        \   columns = \"PrimaryPropertyType\", \n                                \
        \                                        aggfunc = lambda X: X.quantile(0.5))\n\
        \n    # Fill NAs\n    for index in (clean_data.loc[clean_data[\"NumberofFloors\"\
        ].isna(), \"NumberofFloors\"]).index.tolist():\n\n        row_neighborhood\
        \ = emission_df.loc[emission_df.index == index, \"Neighborhood\"].at[index]\n\
        \        row_property_type = emission_df.loc[emission_df.index == index, \"\
        PrimaryPropertyType\"].at[index]\n\n        clean_data.loc[clean_data.index\
        \ ==  index, \"NumberofFloors\"] = \\\n            fetch_noFloors(emission_df,\
        \ row_neighborhood, row_property_type)\n\n    clean_data.loc[clean_data[\"\
        ENERGYSTARScore\"].isna(), \"ENERGYSTARScore\"] = 0\n    clean_data.reset_index(inplace\
        \ = True, drop = True)\n\n    clean_data.to_csv(output_cleandatacsv, index=False)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Preprocess dataset',\
        \ description='')\n_parser.add_argument(\"--input-edfcsv\", dest=\"input_edfcsv\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --output-cleandatacsv\", dest=\"output_cleandatacsv\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = preprocess_dataset(**_parsed_args)\n"
      image: public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest
    inputs:
      artifacts:
      - {name: merge-and-split-output_edfcsv, path: /tmp/inputs/input_edfcsv/data}
    outputs:
      artifacts:
      - {name: preprocess-dataset-output_cleandatacsv, path: /tmp/outputs/output_cleandatacsv/data}
    metadata:
      annotations: {author: Antoine Villatte, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--input-edfcsv", {"inputPath": "input_edfcsv"},
          "--output-cleandatacsv", {"outputPath": "output_cleandatacsv"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef preprocess_dataset(input_edfcsv,\n                        output_cleandatacsv):\n\n    import
          pandas as pd\n    import numpy as np\n    from scipy.stats import boxcox\n    import
          re\n    import math\n\n    def count_commas(text):\n        \"\"\"Fonction
          text.count(\",\") sachant g\u00e9rer les nan\"\"\"\n        if type(text)
          is str :\n            return(text.count(\",\") + 1)\n        else :\n            if
          np.isnan(text):\n                return(np.nan)\n            else :\n                raise(TypeError(\"Valeur
          non textuelle et non NA observ\u00e9e\"))\n\n    def fetch_noFloors(emission_df,
          neighborhood, property_type):\n        neighb_prop_pivot_table = emission_df.dropna().loc[emission_df[\"in_train\"]
          == 1, :].pivot_table(\"NumberofFloors\", \n                                                                   index
          = \"Neighborhood\", \n                                                                   columns
          = \"PrimaryPropertyType\", \n                                                                   aggfunc
          = lambda X: X.quantile(0.5))\n        output = neighb_prop_pivot_table.loc[neighborhood,
          property_type]\n        if math.isnan(output):\n            print(\"Pas
          de donn\u00e9es pour ce type de b\u00e2timent dans ce voisinage, extraction
          de la m\u00e9diane globale\")\n            output = round(neighb_prop_pivot_table.loc[:,
          property_type].quantile(0.5),0)\n\n        return(output)\n\n    emission_df
          = pd.read_csv(input_edfcsv)\n\n    # Log of response variable\n    emission_df[\"BCResponse\"]
          = boxcox(emission_df[\"TotalGHGEmissions\"] + 1, -0.1)\n\n    # Fill NAs\n        ##
          Impute SecondLargestPropertyUseType\n    emission_df.loc[(emission_df[\"SecondLargestPropertyUseType\"].isna())
          & (~emission_df[\"ListOfAllPropertyUseTypes\"].isna()), \n                    \"SecondLargestPropertyUseType\"]
          = \"None\"\n\n        ## Impute SecondLargestPropertyUseTypeGFA\n    emission_df.loc[(emission_df[\"SecondLargestPropertyUseTypeGFA\"].isna())
          & (~emission_df[\"ListOfAllPropertyUseTypes\"].isna()), \n                    \"SecondLargestPropertyUseTypeGFA\"]
          = 0\n\n        ## Impute ThirdLargestPropertyUseType\n    emission_df.loc[(emission_df[\"ThirdLargestPropertyUseType\"].isna())
          & (~emission_df[\"ListOfAllPropertyUseTypes\"].isna()), \n                    \"ThirdLargestPropertyUseType\"]
          = \"None\"\n\n        ## Impute ThirdLargestPropertyUseTypeGFA\n    emission_df.loc[(emission_df[\"ThirdLargestPropertyUseTypeGFA\"].isna())
          & (~emission_df[\"ListOfAllPropertyUseTypes\"].isna()), \n                    \"ThirdLargestPropertyUseTypeGFA\"]
          = 0\n\n    # Drop columns with too many NaNs\n    emission_df.drop(columns
          = [\"YearsENERGYSTARCertified\", \"Comments\", \"Outlier\"], inplace = True)\n\n    #
          Only keep non residential buildings\n    emission_df = emission_df.loc[emission_df[\"BuildingType\"].apply(lambda
          X: bool(re.search(\"^[Nn]on[Rr]esidential\", X))), :]\n    # Drop Building
          Type\n    emission_df.drop(columns = \"BuildingType\", inplace = True)\n\n    #
          Drop low-impact location variables\n    emission_df.drop(columns = [\"Address\",
          \"City\", \"State\", \"ZipCode\", \"TaxParcelIdentificationNumber\", \"CouncilDistrictCode\"],
          \n                    inplace = True)\n\n    # Drop ID variables\n    emission_df.drop(columns
          = [\"OSEBuildingID\", \"PropertyName\", \"DefaultData\"], inplace = True)\n\n    #
          Reset index\n    emission_df.reset_index(inplace = True, drop = True)\n\n    #
          Drop variables too highly correlated to the reponse variable\n    emission_df.drop(columns
          = [''SiteEUIWN(kBtu/sf)'', ''SourceEUI(kBtu/sf)'', ''SourceEUIWN(kBtu/sf)'',\n                                ''SiteEnergyUseWN(kBtu)'',
          ''Electricity(kWh)'', ''NaturalGas(therms)'', \n                                \"SiteEUI(kBtu/sf)\",
          \"GHGEmissionsIntensity\"], inplace = True)\n\n    # Feature engineering
          : neighborhood type\n    emission_df[\"Neighborhood_type_GHGE\"] = \"med-low\"\n    emission_df.loc[emission_df[\"Neighborhood\"].isin([\"GREATER
          DUWAMISH\", \"SOUTHEAST\"]), \"Neighborhood_type_GHGE\"] = \"low\"\n    emission_df.loc[emission_df[\"Neighborhood\"].isin([\"NORTHEAST\",
          \"MAGNOLIA / QUEEN ANNE\", \"SOUTHWEST\", \"LAKE UNION\"]), \n                    \"Neighborhood_type_GHGE\"]
          = \"med-high\"\n    emission_df.loc[emission_df[\"Neighborhood\"].isin([\"DOWNTOWN\",
          \"EAST\"]), \"Neighborhood_type_GHGE\"] = \"high\"\n\n    # Feature engineering
          : age\n    emission_df[\"Age\"] = emission_df[\"DataYear\"] - emission_df[\"YearBuilt\"]\n\n    #
          FE : Number of use types\n    emission_df[\"NumberOfUseTypes\"] = emission_df[\"ListOfAllPropertyUseTypes\"].apply(count_commas)\n\n    #
          FE : Proportion occupied by each use\n    three_uses_sum = emission_df[\"LargestPropertyUseTypeGFA\"]
          + emission_df[\"SecondLargestPropertyUseTypeGFA\"] + \\\n                    emission_df[\"ThirdLargestPropertyUseTypeGFA\"]\n\n    emission_df[\"PrimaryUseGFARatio\"]
          = round(emission_df[\"LargestPropertyUseTypeGFA\"] / three_uses_sum, 3)\n    emission_df[\"SecondaryUseGFARatio\"]
          = round(emission_df[\"SecondLargestPropertyUseTypeGFA\"] / three_uses_sum,
          3)\n    emission_df[\"TerciaryUseGFARatio\"] = round(emission_df[\"ThirdLargestPropertyUseTypeGFA\"]
          / three_uses_sum, 3)\n\n    # FE : proportion of each energy use\n    emission_df[\"NG_ratio\"]
          = round(emission_df[\"NaturalGas(kBtu)\"] / emission_df[\"SiteEnergyUse(kBtu)\"],
          3)\n    emission_df[\"Elec_ratio\"] = round(emission_df[\"Electricity(kBtu)\"]
          / emission_df[\"SiteEnergyUse(kBtu)\"], 3)\n    emission_df[\"Steam_ratio\"]
          = round(emission_df[\"SteamUse(kBtu)\"] / emission_df[\"SiteEnergyUse(kBtu)\"],
          3)\n    emission_df.drop(index = [780, 2781, 2027, 498, 1677], inplace =
          True) # Impossible values\n    emission_df.reset_index(drop = True, inplace
          = True)\n\n    # Drop useless columns\n    clean_data = emission_df.drop(columns
          = [\"DataYear\", \"Neighborhood\", \"Longitude\", \"Latitude\", \"YearBuilt\",
          \"PropertyGFATotal\", \n                                            \"SteamUse(kBtu)\",
          \"Electricity(kBtu)\", \"NaturalGas(kBtu)\", \"PrimaryPropertyType\"])\n\n    #
          Drop NAs\n    clean_data.dropna(subset = [\"LargestPropertyUseType\", \"TotalGHGEmissions\",
          \"NG_ratio\"], inplace = True)\n    emission_df.dropna().loc[emission_df[\"in_train\"]
          == 1, :].pivot_table(\"NumberofFloors\", \n                                                                        index
          = \"Neighborhood\", \n                                                                        columns
          = \"PrimaryPropertyType\", \n                                                                        aggfunc
          = lambda X: X.quantile(0.5))\n\n    # Fill NAs\n    for index in (clean_data.loc[clean_data[\"NumberofFloors\"].isna(),
          \"NumberofFloors\"]).index.tolist():\n\n        row_neighborhood = emission_df.loc[emission_df.index
          == index, \"Neighborhood\"].at[index]\n        row_property_type = emission_df.loc[emission_df.index
          == index, \"PrimaryPropertyType\"].at[index]\n\n        clean_data.loc[clean_data.index
          ==  index, \"NumberofFloors\"] = \\\n            fetch_noFloors(emission_df,
          row_neighborhood, row_property_type)\n\n    clean_data.loc[clean_data[\"ENERGYSTARScore\"].isna(),
          \"ENERGYSTARScore\"] = 0\n    clean_data.reset_index(inplace = True, drop
          = True)\n\n    clean_data.to_csv(output_cleandatacsv, index=False)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Preprocess dataset'',
          description='''')\n_parser.add_argument(\"--input-edfcsv\", dest=\"input_edfcsv\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-cleandatacsv\",
          dest=\"output_cleandatacsv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = preprocess_dataset(**_parsed_args)\n"], "image": "public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest"}},
          "inputs": [{"name": "input_edfcsv", "type": "CSV"}], "metadata": {"annotations":
          {"author": "Antoine Villatte"}}, "name": "Preprocess dataset", "outputs":
          [{"name": "output_cleandatacsv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "6be827e49ca000f6859aab5a8eb8a9b3dc9fbadb99b7c003887361dc625f2a39", "url":
          "./kf_utils/preprocess_dataset_op.yaml"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: train-best-model
    container:
      args: [--best-model, '{{inputs.parameters.evaluate-models-best_model}}', --best-hyperparams,
        '{{inputs.parameters.evaluate-models-hyperparams}}', --input-X-train-csv,
        /tmp/inputs/input_X_train_csv/data, --input-y-train-csv, /tmp/inputs/input_y_train_csv/data,
        --output-pickle-model, /tmp/outputs/output_pickle_model/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef train_best_model(best_model, \n                     best_hyperparams,\
        \ \n                     input_X_train_csv, \n                     input_y_train_csv,\n\
        \                     output_pickle_model):\n\n    import pandas as pd\n \
        \   import xgboost as xgb\n    from sklearn.metrics import mean_squared_error,\
        \ explained_variance_score, r2_score\n    from sklearn.model_selection import\
        \ cross_val_score, KFold\n    from sklearn.svm import SVR\n    from sklearn.ensemble\
        \ import RandomForestRegressor\n    import os\n    import pickle\n\n    if\
        \ best_model == 'XGB':\n        modfit = xgb.XGBRegressor(objective = \"reg:squarederror\"\
        ,\n                                   tree_method = 'hist',\n            \
        \                       eval_metric = [\"rmse\"],\n                      \
        \             **best_hyperparams)\n    elif best_model == 'SVM':\n       \
        \ modfit = SVR(**best_hyperparams)\n\n    elif best_model == \"RandomForest\"\
        :\n        modfit = RandomForestRegressor(**best_hyperparams)\n\n    else:\n\
        \        raise ValueError(\"Model name not recognized : \".format(best_model))\n\
        \n    X_train = pd.read_csv(input_X_train_csv)\n    y_train = pd.read_csv(input_y_train_csv)\n\
        \    modfit.fit(X_train, y_train)\n\n    with open(output_pickle_model, 'wb')\
        \ as f:\n        pickle.dump(modfit, f)\n\nimport json\nimport argparse\n\
        _parser = argparse.ArgumentParser(prog='Train best model', description='')\n\
        _parser.add_argument(\"--best-model\", dest=\"best_model\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--best-hyperparams\"\
        , dest=\"best_hyperparams\", type=json.loads, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--input-X-train-csv\", dest=\"input_X_train_csv\",\
        \ type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --input-y-train-csv\", dest=\"input_y_train_csv\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-pickle-model\"\
        , dest=\"output_pickle_model\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = train_best_model(**_parsed_args)\n"
      image: public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest
    inputs:
      parameters:
      - {name: evaluate-models-best_model}
      - {name: evaluate-models-hyperparams}
      artifacts:
      - {name: prepare-data-output_xtraincsv, path: /tmp/inputs/input_X_train_csv/data}
      - {name: prepare-data-output_ytraincsv, path: /tmp/inputs/input_y_train_csv/data}
    outputs:
      artifacts:
      - {name: train-best-model-output_pickle_model, path: /tmp/outputs/output_pickle_model/data}
    metadata:
      annotations: {author: Antoine Villatte, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--best-model", {"inputValue": "best_model"}, "--best-hyperparams",
          {"inputValue": "best_hyperparams"}, "--input-X-train-csv", {"inputPath":
          "input_X_train_csv"}, "--input-y-train-csv", {"inputPath": "input_y_train_csv"},
          "--output-pickle-model", {"outputPath": "output_pickle_model"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef train_best_model(best_model, \n                     best_hyperparams,
          \n                     input_X_train_csv, \n                     input_y_train_csv,\n                     output_pickle_model):\n\n    import
          pandas as pd\n    import xgboost as xgb\n    from sklearn.metrics import
          mean_squared_error, explained_variance_score, r2_score\n    from sklearn.model_selection
          import cross_val_score, KFold\n    from sklearn.svm import SVR\n    from
          sklearn.ensemble import RandomForestRegressor\n    import os\n    import
          pickle\n\n    if best_model == ''XGB'':\n        modfit = xgb.XGBRegressor(objective
          = \"reg:squarederror\",\n                                   tree_method
          = ''hist'',\n                                   eval_metric = [\"rmse\"],\n                                   **best_hyperparams)\n    elif
          best_model == ''SVM'':\n        modfit = SVR(**best_hyperparams)\n\n    elif
          best_model == \"RandomForest\":\n        modfit = RandomForestRegressor(**best_hyperparams)\n\n    else:\n        raise
          ValueError(\"Model name not recognized : \".format(best_model))\n\n    X_train
          = pd.read_csv(input_X_train_csv)\n    y_train = pd.read_csv(input_y_train_csv)\n    modfit.fit(X_train,
          y_train)\n\n    with open(output_pickle_model, ''wb'') as f:\n        pickle.dump(modfit,
          f)\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train
          best model'', description='''')\n_parser.add_argument(\"--best-model\",
          dest=\"best_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--best-hyperparams\",
          dest=\"best_hyperparams\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-X-train-csv\",
          dest=\"input_X_train_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-y-train-csv\",
          dest=\"input_y_train_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-pickle-model\",
          dest=\"output_pickle_model\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_best_model(**_parsed_args)\n"], "image": "public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest"}},
          "inputs": [{"name": "best_model", "type": "String"}, {"name": "best_hyperparams",
          "type": "JsonObject"}, {"name": "input_X_train_csv", "type": "CSV"}, {"name":
          "input_y_train_csv", "type": "CSV"}], "metadata": {"annotations": {"author":
          "Antoine Villatte"}}, "name": "Train best model", "outputs": [{"name": "output_pickle_model",
          "type": "Pickle"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "1a8a3c3b40092fc87b74ec45aa59026cfbaeb71bfc20721f1cdd2dc353e32745", "url":
          "./kf_utils/train_best_model_op.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"best_hyperparams":
          "{{inputs.parameters.evaluate-models-hyperparams}}", "best_model": "{{inputs.parameters.evaluate-models-best_model}}"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: train-randomforest
    container:
      args: [--input-x-train-csv, /tmp/inputs/input_x_train_csv/data, --input-y-train-csv,
        /tmp/inputs/input_y_train_csv/data, --input-x-test-csv, /tmp/inputs/input_x_test_csv/data,
        --input-y-test-csv, /tmp/inputs/input_y_test_csv/data, --hyperopt-iterations,
        '{{inputs.parameters.hyperopt_iterations}}', '----output-paths', /tmp/outputs/MSE/data,
        /tmp/outputs/R2/data, /tmp/outputs/hyperparams/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def train_randomforest(input_x_train_csv, \n                       input_y_train_csv,\
        \ \n                       input_x_test_csv, \n                       input_y_test_csv,\n\
        \                      hyperopt_iterations\n  ):\n\n    global best\n\n  \
        \  import pandas as pd\n    from sklearn.ensemble import RandomForestRegressor\n\
        \    from sklearn.metrics import mean_squared_error, r2_score\n    from sklearn.model_selection\
        \ import cross_val_score\n    import hyperopt\n    from hyperopt import fmin,\
        \ tpe, hp, STATUS_OK, Trials, space_eval\n    from random import seed\n\n\
        \    X_train = pd.read_csv(input_x_train_csv)\n    y_train = pd.read_csv(input_y_train_csv)\n\
        \    X_test = pd.read_csv(input_x_test_csv)\n    y_test = pd.read_csv(input_y_test_csv)\n\
        \n    seed(42)\n    def model_accuracy(params):\n        rf_reg = RandomForestRegressor(**params)\n\
        \        return cross_val_score(rf_reg, X_train, y_train).mean()\n\n    space\
        \ = {\n        'max_depth': hp.choice('max_depth', range(1, 20)),\n      \
        \  'max_features': hp.choice('max_features', range(1, 70)),\n        'n_estimators':\
        \ hp.choice('n_estimators', range(100, 500)),\n        'min_samples_split'\
        \ : hp.choice('min_samples_split', range(2, 10)),\n        'min_samples_leaf'\
        \ : hp.choice('min_samples_leaf', range(1, 10)),\n        'max_leaf_nodes'\
        \ : hp.choice('max_leaf_nodes', range(2, 10, 2)),\n        'criterion': hp.choice('criterion',\
        \ [\"mse\", \"mae\"])}\n\n    best=0\n    def hyperparameter_tuning(space):\n\
        \        global best\n        acc = model_accuracy(space)\n\n        if acc\
        \ > best:\n            best = acc\n            print ('new best:', best, space)\n\
        \n        return {'loss': 1-acc, 'status': STATUS_OK}\n\n    trials = Trials()\n\
        \    best = fmin(hyperparameter_tuning, space, algo=tpe.suggest, max_evals=hyperopt_iterations,\
        \ trials=trials)\n\n    rf_hyperparams = space_eval(space, best)\n\n    modfit_rf\
        \ = RandomForestRegressor(**rf_hyperparams)\n    modfit_rf.fit(X_train, y_train)\n\
        \n    rf_mse = mean_squared_error(y_test.to_numpy(), modfit_rf.predict(X_test))\n\
        \    rf_accuracies = cross_val_score(estimator = modfit_rf, X = X_test, y\
        \ = y_test, \n                                    cv = 10)\n    rf_r2 = rf_accuracies.mean()\n\
        \n    return(rf_mse, rf_r2, rf_hyperparams)\n\ndef _serialize_float(float_value:\
        \ float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n\
        \    if not isinstance(float_value, (float, int)):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of float.'.format(\n            str(float_value),\
        \ str(type(float_value))))\n    return str(float_value)\n\ndef _serialize_json(obj)\
        \ -> str:\n    if isinstance(obj, str):\n        return obj\n    import json\n\
        \n    def default_serializer(obj):\n        if hasattr(obj, 'to_struct'):\n\
        \            return obj.to_struct()\n        else:\n            raise TypeError(\n\
        \                \"Object of type '%s' is not JSON serializable and does not\
        \ have .to_struct() method.\"\n                % obj.__class__.__name__)\n\
        \n    return json.dumps(obj, default=default_serializer, sort_keys=True)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Train randomforest',\
        \ description='')\n_parser.add_argument(\"--input-x-train-csv\", dest=\"input_x_train_csv\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --input-y-train-csv\", dest=\"input_y_train_csv\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-x-test-csv\"\
        , dest=\"input_x_test_csv\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--input-y-test-csv\", dest=\"input_y_test_csv\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--hyperopt-iterations\"\
        , dest=\"hyperopt_iterations\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = train_randomforest(**_parsed_args)\n\n\
        _output_serializers = [\n    _serialize_float,\n    _serialize_float,\n  \
        \  _serialize_json,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest
    inputs:
      parameters:
      - {name: hyperopt_iterations}
      artifacts:
      - {name: prepare-data-output_xtestcsv, path: /tmp/inputs/input_x_test_csv/data}
      - {name: prepare-data-output_xtraincsv, path: /tmp/inputs/input_x_train_csv/data}
      - {name: prepare-data-output_ytestcsv, path: /tmp/inputs/input_y_test_csv/data}
      - {name: prepare-data-output_ytraincsv, path: /tmp/inputs/input_y_train_csv/data}
    outputs:
      parameters:
      - name: train-randomforest-MSE
        valueFrom: {path: /tmp/outputs/MSE/data}
      - name: train-randomforest-R2
        valueFrom: {path: /tmp/outputs/R2/data}
      - name: train-randomforest-hyperparams
        valueFrom: {path: /tmp/outputs/hyperparams/data}
      artifacts:
      - {name: train-randomforest-MSE, path: /tmp/outputs/MSE/data}
      - {name: train-randomforest-R2, path: /tmp/outputs/R2/data}
      - {name: train-randomforest-hyperparams, path: /tmp/outputs/hyperparams/data}
    metadata:
      annotations: {author: Antoine Villatte, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--input-x-train-csv", {"inputPath": "input_x_train_csv"},
          "--input-y-train-csv", {"inputPath": "input_y_train_csv"}, "--input-x-test-csv",
          {"inputPath": "input_x_test_csv"}, "--input-y-test-csv", {"inputPath": "input_y_test_csv"},
          "--hyperopt-iterations", {"inputValue": "hyperopt_iterations"}, "----output-paths",
          {"outputPath": "MSE"}, {"outputPath": "R2"}, {"outputPath": "hyperparams"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def train_randomforest(input_x_train_csv,
          \n                       input_y_train_csv, \n                       input_x_test_csv,
          \n                       input_y_test_csv,\n                      hyperopt_iterations\n  ):\n\n    global
          best\n\n    import pandas as pd\n    from sklearn.ensemble import RandomForestRegressor\n    from
          sklearn.metrics import mean_squared_error, r2_score\n    from sklearn.model_selection
          import cross_val_score\n    import hyperopt\n    from hyperopt import fmin,
          tpe, hp, STATUS_OK, Trials, space_eval\n    from random import seed\n\n    X_train
          = pd.read_csv(input_x_train_csv)\n    y_train = pd.read_csv(input_y_train_csv)\n    X_test
          = pd.read_csv(input_x_test_csv)\n    y_test = pd.read_csv(input_y_test_csv)\n\n    seed(42)\n    def
          model_accuracy(params):\n        rf_reg = RandomForestRegressor(**params)\n        return
          cross_val_score(rf_reg, X_train, y_train).mean()\n\n    space = {\n        ''max_depth'':
          hp.choice(''max_depth'', range(1, 20)),\n        ''max_features'': hp.choice(''max_features'',
          range(1, 70)),\n        ''n_estimators'': hp.choice(''n_estimators'', range(100,
          500)),\n        ''min_samples_split'' : hp.choice(''min_samples_split'',
          range(2, 10)),\n        ''min_samples_leaf'' : hp.choice(''min_samples_leaf'',
          range(1, 10)),\n        ''max_leaf_nodes'' : hp.choice(''max_leaf_nodes'',
          range(2, 10, 2)),\n        ''criterion'': hp.choice(''criterion'', [\"mse\",
          \"mae\"])}\n\n    best=0\n    def hyperparameter_tuning(space):\n        global
          best\n        acc = model_accuracy(space)\n\n        if acc > best:\n            best
          = acc\n            print (''new best:'', best, space)\n\n        return
          {''loss'': 1-acc, ''status'': STATUS_OK}\n\n    trials = Trials()\n    best
          = fmin(hyperparameter_tuning, space, algo=tpe.suggest, max_evals=hyperopt_iterations,
          trials=trials)\n\n    rf_hyperparams = space_eval(space, best)\n\n    modfit_rf
          = RandomForestRegressor(**rf_hyperparams)\n    modfit_rf.fit(X_train, y_train)\n\n    rf_mse
          = mean_squared_error(y_test.to_numpy(), modfit_rf.predict(X_test))\n    rf_accuracies
          = cross_val_score(estimator = modfit_rf, X = X_test, y = y_test, \n                                    cv
          = 10)\n    rf_r2 = rf_accuracies.mean()\n\n    return(rf_mse, rf_r2, rf_hyperparams)\n\ndef
          _serialize_float(float_value: float) -> str:\n    if isinstance(float_value,
          str):\n        return float_value\n    if not isinstance(float_value, (float,
          int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          float.''.format(\n            str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\ndef _serialize_json(obj) -> str:\n    if isinstance(obj,
          str):\n        return obj\n    import json\n\n    def default_serializer(obj):\n        if
          hasattr(obj, ''to_struct''):\n            return obj.to_struct()\n        else:\n            raise
          TypeError(\n                \"Object of type ''%s'' is not JSON serializable
          and does not have .to_struct() method.\"\n                % obj.__class__.__name__)\n\n    return
          json.dumps(obj, default=default_serializer, sort_keys=True)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train randomforest'', description='''')\n_parser.add_argument(\"--input-x-train-csv\",
          dest=\"input_x_train_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-y-train-csv\",
          dest=\"input_y_train_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-x-test-csv\",
          dest=\"input_x_test_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-y-test-csv\",
          dest=\"input_y_test_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--hyperopt-iterations\",
          dest=\"hyperopt_iterations\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = train_randomforest(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_float,\n    _serialize_float,\n    _serialize_json,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest"}}, "inputs": [{"name":
          "input_x_train_csv", "type": "CSV"}, {"name": "input_y_train_csv", "type":
          "CSV"}, {"name": "input_x_test_csv", "type": "CSV"}, {"name": "input_y_test_csv",
          "type": "CSV"}, {"name": "hyperopt_iterations", "type": "Integer"}], "metadata":
          {"annotations": {"author": "Antoine Villatte"}}, "name": "Train randomforest",
          "outputs": [{"name": "MSE", "type": "Float"}, {"name": "R2", "type": "Float"},
          {"name": "hyperparams", "type": "JsonObject"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "d409f2ef67c47aca2ba39faaa5862da7432f0fb64be28b81fd400062f79067c4", "url":
          "./kf_utils/train_randomforest_op.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"hyperopt_iterations":
          "{{inputs.parameters.hyperopt_iterations}}"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: train-svm
    container:
      args: [--input-x-train-csv, /tmp/inputs/input_x_train_csv/data, --input-y-train-csv,
        /tmp/inputs/input_y_train_csv/data, --input-x-test-csv, /tmp/inputs/input_x_test_csv/data,
        --input-y-test-csv, /tmp/inputs/input_y_test_csv/data, --hyperopt-iterations,
        '{{inputs.parameters.hyperopt_iterations}}', '----output-paths', /tmp/outputs/MSE/data,
        /tmp/outputs/R2/data, /tmp/outputs/hyperparams/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def train_svm(input_x_train_csv, \n              input_y_train_csv, \n  \
        \            input_x_test_csv, \n              input_y_test_csv,\n       \
        \       hyperopt_iterations\n  ):\n\n    global best\n\n    import pandas\
        \ as pd\n    import xgboost as xgb\n    from sklearn.metrics import mean_squared_error,\
        \ explained_variance_score, r2_score\n    from sklearn.model_selection import\
        \ cross_val_score, KFold\n    from sklearn.svm import SVR\n    import hyperopt\n\
        \    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n \
        \   from random import seed\n\n    X_train = pd.read_csv(input_x_train_csv)\n\
        \    y_train = pd.read_csv(input_y_train_csv)\n    X_test = pd.read_csv(input_x_test_csv)\n\
        \    y_test = pd.read_csv(input_y_test_csv)\n\n    seed(42)\n    def model_accuracy(params):\n\
        \        svm_reg = SVR(**params)\n        return cross_val_score(svm_reg,\
        \ X_train, y_train).mean()\n\n    space = {\n        'C': hp.quniform('C',\
        \ 0.005, 1.0, 0.01),\n        'degree': hp.choice('degree', range(2, 6)),\n\
        \        'coef0' : hp.quniform('coef0', 0.5, 2, 0.2),\n        'gamma' : hp.quniform('gamma',\
        \ 0.005, 0.1, 0.01),\n        'kernel': hp.choice('kernel', [\"linear\", \"\
        rbf\", \"sigmoid\"])\n    }\n\n    best=0\n    def hyperparameter_tuning(space):\n\
        \        global best\n        acc = model_accuracy(space)\n\n        if acc\
        \ > best:\n            best = acc\n            print ('new best:', best, space)\n\
        \n        return {'loss': 1-acc, 'status': STATUS_OK}\n\n    trials = Trials()\n\
        \    best = fmin(hyperparameter_tuning, space, algo=tpe.suggest, max_evals=hyperopt_iterations,\
        \ trials=trials)\n\n    svm_hyperparams = space_eval(space, best)\n\n    modfit_svm\
        \ = SVR(**svm_hyperparams)\n\n    modfit_svm.fit(X_train, y_train)\n\n   \
        \ svm_mse = mean_squared_error(y_test.to_numpy(), modfit_svm.predict(X_test))\n\
        \    svm_accuracies = cross_val_score(estimator = modfit_svm, X = X_test,\
        \ y = y_test, \n                                    cv = 10)\n    svm_r2 =\
        \ svm_accuracies.mean()\n\n    return(svm_mse, svm_r2, svm_hyperparams)\n\n\
        def _serialize_float(float_value: float) -> str:\n    if isinstance(float_value,\
        \ str):\n        return float_value\n    if not isinstance(float_value, (float,\
        \ int)):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ float.'.format(\n            str(float_value), str(type(float_value))))\n\
        \    return str(float_value)\n\ndef _serialize_json(obj) -> str:\n    if isinstance(obj,\
        \ str):\n        return obj\n    import json\n\n    def default_serializer(obj):\n\
        \        if hasattr(obj, 'to_struct'):\n            return obj.to_struct()\n\
        \        else:\n            raise TypeError(\n                \"Object of\
        \ type '%s' is not JSON serializable and does not have .to_struct() method.\"\
        \n                % obj.__class__.__name__)\n\n    return json.dumps(obj,\
        \ default=default_serializer, sort_keys=True)\n\nimport argparse\n_parser\
        \ = argparse.ArgumentParser(prog='Train svm', description='')\n_parser.add_argument(\"\
        --input-x-train-csv\", dest=\"input_x_train_csv\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-y-train-csv\"\
        , dest=\"input_y_train_csv\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--input-x-test-csv\", dest=\"input_x_test_csv\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-y-test-csv\"\
        , dest=\"input_y_test_csv\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--hyperopt-iterations\", dest=\"hyperopt_iterations\"\
        , type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = train_svm(**_parsed_args)\n\n_output_serializers = [\n\
        \    _serialize_float,\n    _serialize_float,\n    _serialize_json,\n\n]\n\
        \nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n\
        \        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest
    inputs:
      parameters:
      - {name: hyperopt_iterations}
      artifacts:
      - {name: prepare-data-output_xtestcsv, path: /tmp/inputs/input_x_test_csv/data}
      - {name: prepare-data-output_xtraincsv, path: /tmp/inputs/input_x_train_csv/data}
      - {name: prepare-data-output_ytestcsv, path: /tmp/inputs/input_y_test_csv/data}
      - {name: prepare-data-output_ytraincsv, path: /tmp/inputs/input_y_train_csv/data}
    outputs:
      parameters:
      - name: train-svm-MSE
        valueFrom: {path: /tmp/outputs/MSE/data}
      - name: train-svm-R2
        valueFrom: {path: /tmp/outputs/R2/data}
      - name: train-svm-hyperparams
        valueFrom: {path: /tmp/outputs/hyperparams/data}
      artifacts:
      - {name: train-svm-MSE, path: /tmp/outputs/MSE/data}
      - {name: train-svm-R2, path: /tmp/outputs/R2/data}
      - {name: train-svm-hyperparams, path: /tmp/outputs/hyperparams/data}
    metadata:
      annotations: {author: Antoine Villatte, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--input-x-train-csv", {"inputPath": "input_x_train_csv"},
          "--input-y-train-csv", {"inputPath": "input_y_train_csv"}, "--input-x-test-csv",
          {"inputPath": "input_x_test_csv"}, "--input-y-test-csv", {"inputPath": "input_y_test_csv"},
          "--hyperopt-iterations", {"inputValue": "hyperopt_iterations"}, "----output-paths",
          {"outputPath": "MSE"}, {"outputPath": "R2"}, {"outputPath": "hyperparams"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def train_svm(input_x_train_csv,
          \n              input_y_train_csv, \n              input_x_test_csv, \n              input_y_test_csv,\n              hyperopt_iterations\n  ):\n\n    global
          best\n\n    import pandas as pd\n    import xgboost as xgb\n    from sklearn.metrics
          import mean_squared_error, explained_variance_score, r2_score\n    from
          sklearn.model_selection import cross_val_score, KFold\n    from sklearn.svm
          import SVR\n    import hyperopt\n    from hyperopt import fmin, tpe, hp,
          STATUS_OK, Trials, space_eval\n    from random import seed\n\n    X_train
          = pd.read_csv(input_x_train_csv)\n    y_train = pd.read_csv(input_y_train_csv)\n    X_test
          = pd.read_csv(input_x_test_csv)\n    y_test = pd.read_csv(input_y_test_csv)\n\n    seed(42)\n    def
          model_accuracy(params):\n        svm_reg = SVR(**params)\n        return
          cross_val_score(svm_reg, X_train, y_train).mean()\n\n    space = {\n        ''C'':
          hp.quniform(''C'', 0.005, 1.0, 0.01),\n        ''degree'': hp.choice(''degree'',
          range(2, 6)),\n        ''coef0'' : hp.quniform(''coef0'', 0.5, 2, 0.2),\n        ''gamma''
          : hp.quniform(''gamma'', 0.005, 0.1, 0.01),\n        ''kernel'': hp.choice(''kernel'',
          [\"linear\", \"rbf\", \"sigmoid\"])\n    }\n\n    best=0\n    def hyperparameter_tuning(space):\n        global
          best\n        acc = model_accuracy(space)\n\n        if acc > best:\n            best
          = acc\n            print (''new best:'', best, space)\n\n        return
          {''loss'': 1-acc, ''status'': STATUS_OK}\n\n    trials = Trials()\n    best
          = fmin(hyperparameter_tuning, space, algo=tpe.suggest, max_evals=hyperopt_iterations,
          trials=trials)\n\n    svm_hyperparams = space_eval(space, best)\n\n    modfit_svm
          = SVR(**svm_hyperparams)\n\n    modfit_svm.fit(X_train, y_train)\n\n    svm_mse
          = mean_squared_error(y_test.to_numpy(), modfit_svm.predict(X_test))\n    svm_accuracies
          = cross_val_score(estimator = modfit_svm, X = X_test, y = y_test, \n                                    cv
          = 10)\n    svm_r2 = svm_accuracies.mean()\n\n    return(svm_mse, svm_r2,
          svm_hyperparams)\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(\n            str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\ndef _serialize_json(obj) -> str:\n    if isinstance(obj,
          str):\n        return obj\n    import json\n\n    def default_serializer(obj):\n        if
          hasattr(obj, ''to_struct''):\n            return obj.to_struct()\n        else:\n            raise
          TypeError(\n                \"Object of type ''%s'' is not JSON serializable
          and does not have .to_struct() method.\"\n                % obj.__class__.__name__)\n\n    return
          json.dumps(obj, default=default_serializer, sort_keys=True)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train svm'', description='''')\n_parser.add_argument(\"--input-x-train-csv\",
          dest=\"input_x_train_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-y-train-csv\",
          dest=\"input_y_train_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-x-test-csv\",
          dest=\"input_x_test_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-y-test-csv\",
          dest=\"input_y_test_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--hyperopt-iterations\",
          dest=\"hyperopt_iterations\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = train_svm(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_float,\n    _serialize_float,\n    _serialize_json,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest"}}, "inputs": [{"name":
          "input_x_train_csv", "type": "CSV"}, {"name": "input_y_train_csv", "type":
          "CSV"}, {"name": "input_x_test_csv", "type": "CSV"}, {"name": "input_y_test_csv",
          "type": "CSV"}, {"name": "hyperopt_iterations", "type": "Integer"}], "metadata":
          {"annotations": {"author": "Antoine Villatte"}}, "name": "Train svm", "outputs":
          [{"name": "MSE", "type": "Float"}, {"name": "R2", "type": "Float"}, {"name":
          "hyperparams", "type": "JsonObject"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "e26a807469ad3a858e3d839e62cda2f21a9b2695bdd7b0f18ff010d6d4c98715", "url":
          "./kf_utils/train_svm_op.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"hyperopt_iterations":
          "{{inputs.parameters.hyperopt_iterations}}"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: train-xgb
    container:
      args: [--input-x-train-csv, /tmp/inputs/input_x_train_csv/data, --input-y-train-csv,
        /tmp/inputs/input_y_train_csv/data, --input-x-test-csv, /tmp/inputs/input_x_test_csv/data,
        --input-y-test-csv, /tmp/inputs/input_y_test_csv/data, --hyperopt-iterations,
        '{{inputs.parameters.hyperopt_iterations}}', '----output-paths', /tmp/outputs/MSE/data,
        /tmp/outputs/R2/data, /tmp/outputs/hyperparams/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def train_xgb(input_x_train_csv, \n              input_y_train_csv, \n  \
        \            input_x_test_csv, \n              input_y_test_csv,\n       \
        \       hyperopt_iterations\n  ):\n\n    global best\n\n    import pandas\
        \ as pd\n    import xgboost as xgb\n    from sklearn.metrics import mean_squared_error,\
        \ explained_variance_score, r2_score\n    from sklearn.model_selection import\
        \ cross_val_score, KFold\n    from sklearn.svm import SVR\n    import hyperopt\n\
        \    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n \
        \   from random import seed\n\n    X_train = pd.read_csv(input_x_train_csv)\n\
        \    y_train = pd.read_csv(input_y_train_csv)\n    X_test = pd.read_csv(input_x_test_csv)\n\
        \    y_test = pd.read_csv(input_y_test_csv)\n\n    seed(42)\n    def model_accuracy(params):\n\
        \        xgb_reg = xgb.XGBRegressor(objective = \"reg:squarederror\",\n  \
        \                                 tree_method = 'hist',\n                \
        \                   eval_metric = [\"rmse\"],\n                          \
        \         **params)\n        return cross_val_score(xgb_reg, X_train, y_train).mean()\n\
        \n    space = {\n        'max_depth' : hp.choice('max_depth', range(1, 30,\
        \ 1)),\n        'learning_rate' : hp.quniform('learning_rate', 0.01, 0.5,\
        \ 0.01),\n        'n_estimators' : hp.choice('n_estimators', range(20, 205,\
        \ 5)),\n        'gamma' : hp.quniform('gamma', 0, 0.50, 0.01),\n        'min_child_weight'\
        \ : hp.quniform('min_child_weight', 1, 10, 1),\n        'subsample' : hp.quniform('subsample',\
        \ 0.1, 1, 0.01),\n        'colsample_bytree' : hp.quniform('colsample_bytree',\
        \ 0.1, 1.0, 0.01),\n        'colsample_bylevel' : hp.quniform('colsample_bylevel',\
        \ 0.1, 1.0, 0.01),\n        'colsample_bynode' : hp.quniform('colsample_bynode',\
        \ 0.1, 1.0, 0.01),\n        'max_delta_step' : hp.choice('max_delta_step',\
        \ range(0, 11, 1))\n    }\n\n    best=0\n    def hyperparameter_tuning(space):\n\
        \        global best\n        acc = model_accuracy(space)\n\n        if acc\
        \ > best:\n            best = acc\n            print ('new best:', best, space)\n\
        \n        return {'loss': 1-acc, 'status': STATUS_OK}\n\n    trials = Trials()\n\
        \    best = fmin(hyperparameter_tuning, space, algo=tpe.suggest, max_evals=hyperopt_iterations,\
        \ trials=trials)\n\n    xgb_hyperparams = space_eval(space, best)\n\n    modfit_xgb\
        \ = xgb.XGBRegressor(objective = \"reg:squarederror\",\n                 \
        \                     tree_method = 'hist',\n                            \
        \          eval_metric = [\"rmse\"],\n                                   \
        \   **xgb_hyperparams)\n\n    modfit_xgb.fit(X_train, y_train)\n\n    xgb_mse\
        \ = mean_squared_error(y_test.to_numpy(), modfit_xgb.predict(X_test))\n  \
        \  xgb_accuracies = cross_val_score(estimator = modfit_xgb, X = X_test, y\
        \ = y_test, \n                                    cv = 10)\n    xgb_r2 = xgb_accuracies.mean()\n\
        \n    return(xgb_mse, xgb_r2, xgb_hyperparams)\n\ndef _serialize_float(float_value:\
        \ float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n\
        \    if not isinstance(float_value, (float, int)):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of float.'.format(\n            str(float_value),\
        \ str(type(float_value))))\n    return str(float_value)\n\ndef _serialize_json(obj)\
        \ -> str:\n    if isinstance(obj, str):\n        return obj\n    import json\n\
        \n    def default_serializer(obj):\n        if hasattr(obj, 'to_struct'):\n\
        \            return obj.to_struct()\n        else:\n            raise TypeError(\n\
        \                \"Object of type '%s' is not JSON serializable and does not\
        \ have .to_struct() method.\"\n                % obj.__class__.__name__)\n\
        \n    return json.dumps(obj, default=default_serializer, sort_keys=True)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Train xgb', description='')\n\
        _parser.add_argument(\"--input-x-train-csv\", dest=\"input_x_train_csv\",\
        \ type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --input-y-train-csv\", dest=\"input_y_train_csv\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-x-test-csv\"\
        , dest=\"input_x_test_csv\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--input-y-test-csv\", dest=\"input_y_test_csv\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--hyperopt-iterations\"\
        , dest=\"hyperopt_iterations\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = train_xgb(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_float,\n    _serialize_float,\n    _serialize_json,\n\
        \n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n\
        \        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest
    inputs:
      parameters:
      - {name: hyperopt_iterations}
      artifacts:
      - {name: prepare-data-output_xtestcsv, path: /tmp/inputs/input_x_test_csv/data}
      - {name: prepare-data-output_xtraincsv, path: /tmp/inputs/input_x_train_csv/data}
      - {name: prepare-data-output_ytestcsv, path: /tmp/inputs/input_y_test_csv/data}
      - {name: prepare-data-output_ytraincsv, path: /tmp/inputs/input_y_train_csv/data}
    outputs:
      parameters:
      - name: train-xgb-MSE
        valueFrom: {path: /tmp/outputs/MSE/data}
      - name: train-xgb-R2
        valueFrom: {path: /tmp/outputs/R2/data}
      - name: train-xgb-hyperparams
        valueFrom: {path: /tmp/outputs/hyperparams/data}
      artifacts:
      - {name: train-xgb-MSE, path: /tmp/outputs/MSE/data}
      - {name: train-xgb-R2, path: /tmp/outputs/R2/data}
      - {name: train-xgb-hyperparams, path: /tmp/outputs/hyperparams/data}
    metadata:
      annotations: {author: Antoine Villatte, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--input-x-train-csv", {"inputPath": "input_x_train_csv"},
          "--input-y-train-csv", {"inputPath": "input_y_train_csv"}, "--input-x-test-csv",
          {"inputPath": "input_x_test_csv"}, "--input-y-test-csv", {"inputPath": "input_y_test_csv"},
          "--hyperopt-iterations", {"inputValue": "hyperopt_iterations"}, "----output-paths",
          {"outputPath": "MSE"}, {"outputPath": "R2"}, {"outputPath": "hyperparams"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def train_xgb(input_x_train_csv,
          \n              input_y_train_csv, \n              input_x_test_csv, \n              input_y_test_csv,\n              hyperopt_iterations\n  ):\n\n    global
          best\n\n    import pandas as pd\n    import xgboost as xgb\n    from sklearn.metrics
          import mean_squared_error, explained_variance_score, r2_score\n    from
          sklearn.model_selection import cross_val_score, KFold\n    from sklearn.svm
          import SVR\n    import hyperopt\n    from hyperopt import fmin, tpe, hp,
          STATUS_OK, Trials, space_eval\n    from random import seed\n\n    X_train
          = pd.read_csv(input_x_train_csv)\n    y_train = pd.read_csv(input_y_train_csv)\n    X_test
          = pd.read_csv(input_x_test_csv)\n    y_test = pd.read_csv(input_y_test_csv)\n\n    seed(42)\n    def
          model_accuracy(params):\n        xgb_reg = xgb.XGBRegressor(objective =
          \"reg:squarederror\",\n                                   tree_method =
          ''hist'',\n                                   eval_metric = [\"rmse\"],\n                                   **params)\n        return
          cross_val_score(xgb_reg, X_train, y_train).mean()\n\n    space = {\n        ''max_depth''
          : hp.choice(''max_depth'', range(1, 30, 1)),\n        ''learning_rate''
          : hp.quniform(''learning_rate'', 0.01, 0.5, 0.01),\n        ''n_estimators''
          : hp.choice(''n_estimators'', range(20, 205, 5)),\n        ''gamma'' : hp.quniform(''gamma'',
          0, 0.50, 0.01),\n        ''min_child_weight'' : hp.quniform(''min_child_weight'',
          1, 10, 1),\n        ''subsample'' : hp.quniform(''subsample'', 0.1, 1, 0.01),\n        ''colsample_bytree''
          : hp.quniform(''colsample_bytree'', 0.1, 1.0, 0.01),\n        ''colsample_bylevel''
          : hp.quniform(''colsample_bylevel'', 0.1, 1.0, 0.01),\n        ''colsample_bynode''
          : hp.quniform(''colsample_bynode'', 0.1, 1.0, 0.01),\n        ''max_delta_step''
          : hp.choice(''max_delta_step'', range(0, 11, 1))\n    }\n\n    best=0\n    def
          hyperparameter_tuning(space):\n        global best\n        acc = model_accuracy(space)\n\n        if
          acc > best:\n            best = acc\n            print (''new best:'', best,
          space)\n\n        return {''loss'': 1-acc, ''status'': STATUS_OK}\n\n    trials
          = Trials()\n    best = fmin(hyperparameter_tuning, space, algo=tpe.suggest,
          max_evals=hyperopt_iterations, trials=trials)\n\n    xgb_hyperparams = space_eval(space,
          best)\n\n    modfit_xgb = xgb.XGBRegressor(objective = \"reg:squarederror\",\n                                      tree_method
          = ''hist'',\n                                      eval_metric = [\"rmse\"],\n                                      **xgb_hyperparams)\n\n    modfit_xgb.fit(X_train,
          y_train)\n\n    xgb_mse = mean_squared_error(y_test.to_numpy(), modfit_xgb.predict(X_test))\n    xgb_accuracies
          = cross_val_score(estimator = modfit_xgb, X = X_test, y = y_test, \n                                    cv
          = 10)\n    xgb_r2 = xgb_accuracies.mean()\n\n    return(xgb_mse, xgb_r2,
          xgb_hyperparams)\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(\n            str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\ndef _serialize_json(obj) -> str:\n    if isinstance(obj,
          str):\n        return obj\n    import json\n\n    def default_serializer(obj):\n        if
          hasattr(obj, ''to_struct''):\n            return obj.to_struct()\n        else:\n            raise
          TypeError(\n                \"Object of type ''%s'' is not JSON serializable
          and does not have .to_struct() method.\"\n                % obj.__class__.__name__)\n\n    return
          json.dumps(obj, default=default_serializer, sort_keys=True)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train xgb'', description='''')\n_parser.add_argument(\"--input-x-train-csv\",
          dest=\"input_x_train_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-y-train-csv\",
          dest=\"input_y_train_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-x-test-csv\",
          dest=\"input_x_test_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-y-test-csv\",
          dest=\"input_y_test_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--hyperopt-iterations\",
          dest=\"hyperopt_iterations\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = train_xgb(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_float,\n    _serialize_float,\n    _serialize_json,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest"}}, "inputs": [{"name":
          "input_x_train_csv", "type": "CSV"}, {"name": "input_y_train_csv", "type":
          "CSV"}, {"name": "input_x_test_csv", "type": "CSV"}, {"name": "input_y_test_csv",
          "type": "CSV"}, {"name": "hyperopt_iterations", "type": "Integer"}], "metadata":
          {"annotations": {"author": "Antoine Villatte"}}, "name": "Train xgb", "outputs":
          [{"name": "MSE", "type": "Float"}, {"name": "R2", "type": "Float"}, {"name":
          "hyperparams", "type": "JsonObject"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "54726d39870ed5af8bab1e59e028e9ca5571546fc01ba3174af4337f85a24981", "url":
          "./kf_utils/train_xgb_op.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"hyperopt_iterations":
          "{{inputs.parameters.hyperopt_iterations}}"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  arguments:
    parameters:
    - {name: bucket}
    - {name: data_2015}
    - {name: data_2016}
    - {name: hyperopt_iterations}
    - {name: subfolder}
  serviceAccountName: pipeline-runner
